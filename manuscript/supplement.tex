\documentclass{article}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{pdflscape}
\usepackage{graphicx}
\usepackage{blindtext}
\usepackage{booktabs}
\usepackage{breqn}
\usepackage{algcompatible}
\usepackage{float}
\usepackage{dutchcal}
\usepackage{amsfonts}

\def\bSig\mathbf{\Sigma}
\newcommand{\VS}{V\&S}
\newcommand{\tr}{\mbox{tr}}

\begin{document}

\section*{Supplementary Materials}

\subsection*{Derivations of Full Conditional Distributions}

\subsubsection*{Multivariate Skew-Normal Regression}

Without loss of generality, we derive the full conditional distributions for the multivariate skew-normal regression model component under the assumption that all observations belong to a single cluster. To make the extension to the case where more than one cluster is specified, simply apply these distributional forms to cluster specific parameters and data. Finally, we assume for the moment that we have complete data for all outcomes for each subject. We extend consider the case of missing data in section (INSERT SECTION).

The multivariate skew-normal regression model can be written as follows in matrix form. 
$$\mathbf{Y} = \mathbf{X}\mathbf{B} + \mathbf{t} \boldsymbol\psi^T + \mathbf{E} = \mathbf{X}^* \mathbf{B}^* + \mathbf{E}$$
The matrix $\mathbf{Y}$ is of dimension $n \times J$. For convenience, we define $\mathbf{X}^*$ as a $n \times (P + 1)$ matrix constructed by column binding $\mathbf{t}$ to $\mathbf{X}$, and $\mathbf{B}^*$ as a $(P+1) \times J$ matrix constructed by row binding $\boldsymbol\psi^T$ to $\mathbf{B}$. We assume that $t_i \stackrel{iid}{\sim}T_{[0,\infty})(0,1)$ and that $\mathbf{E}$ is made of row vectors $\boldsymbol\epsilon_i = (\epsilon_{i1},...,\epsilon_{iJ})$ for $i = 1,...,n$, where $ \boldsymbol\epsilon_i \stackrel{iid}{\sim} N_J(0, \boldsymbol\Sigma)$.

The conditional likelihood for this model is given below. 
$$p(\mathbf{Y}|\mathbf{X}^*,\mathbf{B}^*,\boldsymbol\Sigma) \propto |\boldsymbol\Sigma|^{-n/2}\exp \left \{ -\frac{1}{2} \tr(\mathbf{Y} - \mathbf{X}^* \mathbf{B}^*)^T(\mathbf{Y} - \mathbf{X}^* \mathbf{B}^*)\boldsymbol\Sigma^{-1} \right \}$$

We choose conjugate priors for $\mathbf{B}^*$ and $\boldsymbol\Sigma$ as follows. 
$$\boldsymbol\Sigma \sim \text{inverse-Wishart}(\mathbf{V}_0,\nu_0)$$
$$\mathbf{B}^*|\boldsymbol\Sigma \sim MatNorm_{(m+1), p}(\mathbf{B}_0^*,\mathbf{L}_0^{-1},\boldsymbol\Sigma)$$

We now derive the joint posterior distribution of the parameters $\mathbf{B}^*$ and $\boldsymbol\Sigma$.
\begin{align*} 
p(\mathbf{B}^*,\boldsymbol\Sigma|\mathbf{X}^*,\mathbf{Y}) & \propto  p(\mathbf{Y}|\mathbf{X}^*,\mathbf{B}^*,\boldsymbol\Sigma)p(\mathbf{B}^*|\boldsymbol\Sigma)p(\boldsymbol\Sigma)\\
&  \propto |\boldsymbol\Sigma|^{-n/2}\exp \left \{ -\frac{1}{2} \tr \left [(\mathbf{Y} - \mathbf{X^*}\mathbf{B}^*)^T(\mathbf{Y} - \mathbf{X^*}\mathbf{B}^*)\boldsymbol\Sigma^{-1} \right ]\right \} \\
& \times |\boldsymbol\Sigma|^{-(P+1)/2} \exp \left \{ -\frac{1}{2} \tr \left [(\mathbf{B}^* - \mathbf{B}^*_0)^T \mathbf{L}_0(\mathbf{B}^* - \mathbf{B}^*_0)\boldsymbol\Sigma^{-1} \right ]\right \} \\
& \times |\boldsymbol\Sigma|^{(\nu_0 + J + 1)/2} \exp \left \{ -\frac{1}{2} \tr (\mathbf{V}_0 \boldsymbol\Sigma^{-1}) \right \}
\end{align*}

\subsubsection*{Multinomial Logit Regression}

\subsubsection*{Multivariate Normal Conditional Imputation}

The multivariate normal conditional imputation derivations are given for a single cluster without loss of generality. In practice, the data and parameters in this section would be replaced by cluster specific estimates in the case of clustering. 

For a given observation vector $\mathbf{y} \sim N_J(\boldsymbol\mu, \boldsymbol\Sigma)$, we allow for missingness in at most $J - 1$ of the multivariate outcomes through the use of a conditional imputation step embedded within our Gibbs sampler. Suppose $\mathbf{y}$ contains $q$ missing observations and can be partitioned into two vectors $\mathbf{y_{1}}$ and $\mathbf{y_{2}}$ such that $\mathbf{y_{1}}$ is a $q \times 1$ vector of missing observations and $\mathbf{y_{2}}$ is a $(J-q) \times 1$ vector of complete observations. Similarly, partition $\boldsymbol\mu$ and $\boldsymbol\Sigma$ as follows.
$$\boldsymbol\mu = \begin{bmatrix} \boldsymbol\mu_1 \\ \boldsymbol\mu_2 \end{bmatrix} \ \ \ \ \ \boldsymbol\Sigma = \begin{bmatrix} \boldsymbol\Sigma_{11} & \boldsymbol\Sigma_{12} \\ \boldsymbol\Sigma_{21} & \boldsymbol\Sigma_{22} \end{bmatrix}$$

We will use these quantities to derive the conditional distribution $f(\mathbf{y_1}|\mathbf{y_2},\boldsymbol\mu,\boldsymbol\Sigma)$. 
\begin{align*} 
f(\mathbf{y_1}|\mathbf{y_2},\boldsymbol\mu,\boldsymbol\Sigma) & \propto  f(\mathbf{y_1},\mathbf{y_2}|\boldsymbol\mu,\boldsymbol\Sigma) \\
& \propto \exp \left \{ -\frac{1}{2}(\mathbf{y} - \boldsymbol\mu)^T \boldsymbol\Sigma^{-1} (\mathbf{y} - \boldsymbol\mu) \right \} \\
& = \exp \left \{ -\frac{1}{2}\begin{bmatrix} \mathbf{y}_1 - \boldsymbol\mu_1 \\  \mathbf{y}_2 - \boldsymbol\mu_2 \end{bmatrix}^T \boldsymbol\Sigma^{-1} \begin{bmatrix} \mathbf{y}_1 - \boldsymbol\mu_1 \\  \mathbf{y}_2 - \boldsymbol\mu_2 \end{bmatrix} \right \} \\
& =  \exp \left \{ -\frac{1}{2}\begin{bmatrix} \mathbf{y}_1 - \boldsymbol\mu_1 \\  \mathbf{y}_2 - \boldsymbol\mu_2 \end{bmatrix}^T \begin{bmatrix} \boldsymbol\Sigma_{11} & \boldsymbol\Sigma_{12} \\ \boldsymbol\Sigma_{21} & \boldsymbol\Sigma_{22} \end{bmatrix}^{-1} \begin{bmatrix} \mathbf{y}_1 - \boldsymbol\mu_1 \\  \mathbf{y}_2 - \boldsymbol\mu_2 \end{bmatrix} \right \} \\
& = \exp \left \{ -\frac{1}{2}\begin{bmatrix} \mathbf{y}_1 - \boldsymbol\mu_1 \\  \mathbf{y}_2 - \boldsymbol\mu_2 \end{bmatrix}^T \begin{bmatrix} \boldsymbol\Sigma_{11}^* & \boldsymbol\Sigma_{12}^* \\ \boldsymbol\Sigma_{21}^* & \boldsymbol\Sigma_{22}^* \end{bmatrix} \begin{bmatrix} \mathbf{y}_1 - \boldsymbol\mu_1 \\  \mathbf{y}_2 - \boldsymbol\mu_2 \end{bmatrix} \right \} \\
& = \exp \left \{ -\frac{1}{2} \left[ (\mathbf{y}_1 - \boldsymbol\mu_{cond})^T \boldsymbol\Sigma_{cond}^{-1}(\mathbf{y}_1 - \boldsymbol\mu_{cond})\right]\right \} 
\end{align*}
$$\Rightarrow \mathbf{y_1}|\mathbf{y_2},\boldsymbol\mu,\boldsymbol\Sigma \sim N_q(\boldsymbol\mu_{cond},\boldsymbol\Sigma_{cond})$$
$$\boldsymbol\mu_{cond} = \boldsymbol\mu_1 + \boldsymbol\Sigma_{12}\boldsymbol\Sigma_{22}^{-1}(\mathbf{y}_2 - \boldsymbol\mu_2), \ \ \ \ \ \ \boldsymbol\Sigma_{cond} = \boldsymbol\Sigma_{11} - \boldsymbol\Sigma_{12}\boldsymbol\Sigma_{22}^{-1}\boldsymbol\Sigma_{21}$$

The block-wise inversion formula was used to invert $\boldsymbol\Sigma$ according to the following reparameterizations.
\begin{align*}
\boldsymbol\Sigma_{11}^* & = \boldsymbol\Sigma_{11}^{-1} + \boldsymbol\Sigma_{11}^{-1}\boldsymbol\Sigma_{12}(\boldsymbol\Sigma_{22} - \boldsymbol\Sigma_{21}\boldsymbol\Sigma_{11}^{-1}\boldsymbol\Sigma_{12})^{-1}\boldsymbol\Sigma_{21}\boldsymbol\Sigma_{11}^{-1}\\
\boldsymbol\Sigma_{12}^* & = -\boldsymbol\Sigma_{11} \boldsymbol\Sigma_{12}(\boldsymbol\Sigma_{22}-\boldsymbol\Sigma_{21}\boldsymbol\Sigma_{11}^{-1}\boldsymbol\Sigma_{12})^{-1}\\
\boldsymbol\Sigma_{21}^* & = -(\boldsymbol\Sigma_{22} - \boldsymbol\Sigma_{21}\boldsymbol\Sigma_{11}^{-1}\boldsymbol\Sigma_{12})^{-1}\boldsymbol\Sigma_{21}\boldsymbol\Sigma_{11}^{-1}\\
\boldsymbol\Sigma_{22}^* & = (\boldsymbol\Sigma_{22} - \boldsymbol\Sigma_{21}\boldsymbol\Sigma_{11}^{-1}\boldsymbol\Sigma_{12})^{-1}     
\end{align*}

\newpage

\subsection*{Tables}

\begin{algorithm}
\caption{Gibbs Sampler}
\label{alg:MCMC}
\begin{algorithmic}
\small
\linespread{0.5}
    \State $\mathbf{Define} \ n_{iter};\  n_{burn}; \ K; \ \boldsymbol\theta_{init}; \ \boldsymbol\theta_0$
    \State $n_{sim} := n_{iter} - n_{burn}$
    \State $\boldsymbol\theta := \boldsymbol\theta_{init}$
    \For {$\iota = 1,...,n_{sim}$}
        \State \textsc{I. Conditional Imputation}
          \For {$ i = 1,...,n$}
            \State $\mathbf{Draw} \ \mathbf{y}_i^{miss} \ \text{from} \ N_q(\boldsymbol\mu_i^{miss}, \boldsymbol\Sigma_i^{miss})$
            \State $\mathbf{y}_i := \mathbf{y}_i^{miss} \cup \mathbf{y}_i^{obs}$
         \EndFor
        \State \textsc{II. MSN Regression}
          \For {$ k = 1,...,K$}
            \State $n_k := \sum_{i = 1}^n {1}_{z_i = k}$
            \For {$i_k = 1,...,n_k$}
              \State $\mathbf{Draw}\  t_i \ \text{from} \ N_{[0,\infty)}(a_i,A)$
            \EndFor
            \State $\mathbf{X^*}_k := $ \texttt{cbind}$(\mathbf{X}_k,\mathbf{t}_k)$
            \State $\mathbf{Draw} \ \mathbf{B^*}_k \ \text{from} \ \text{MatNorm}(\mathbf{B}_k,\mathbf{L}_k^{-1},\boldsymbol\Sigma_k)$
            \State $\mathbf{Draw} \ \boldsymbol\Sigma_k \ \text{from} \ \text{InvWish}(\nu_k, \mathbf{V}_k)$
          \EndFor
        \State \textsc{III. Multinomial Logit}
          \For {$i = 1,...,n$}
            \For {$k = 1,...,K$}
              \State $\pi_{ik} := P(z_i = k|\mathbf{w}_i,\boldsymbol\delta_k)$
              \State $p_{ik} := P(\mathbf{y}_i|\boldsymbol\beta_k^{*T} \mathbf{x}^*_i,\boldsymbol\Sigma_k)$
            \EndFor
            \State $\mathbf{p}_{z_i} := \frac{\mathbf{p}_i \circ \boldsymbol\pi_i}{\mathbf{p}_i \cdot \boldsymbol\pi_i}$
            \State $\mathbf{Draw} \ z_i \ \text{from} \ \text{Categorical}(\mathbf{p}_{z_i})$
            \For {$k = 1,...,K-1$}
              \State $\mathbf{Draw} \ \boldsymbol\delta_k \ \text{from} \ N(\mathbf{M},\mathbf{S})$
            \EndFor
          \EndFor
        \State $\boldsymbol\theta := \{\mathbf{B^*}, \boldsymbol\Sigma, \mathbf{Z}, \boldsymbol\delta \}$
        \State $\mathbf{Store} \ \boldsymbol\theta$
	  \EndFor
\end{algorithmic}
\end{algorithm}


\begin{landscape}\begin{table}[t]

\caption{\label{tab:unnamed-chunk-5}Model results for simulated data with n = 1,000, J = 4, p = 2, K = 3, r = 2. 1,000 iterations were run with a burn in of 100. Missingness mechanism was MAR and P(miss) = 0. Model results for the multivariate skew normal (MSN) and multivariate normal (MN) mixtures are presented.}
\centering
\fontsize{6}{8}\selectfont
\begin{tabular}{lllllllllll}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{3}{c}{Class 1} & \multicolumn{3}{c}{Class 2} & \multicolumn{3}{c}{Class 3} \\
\cmidrule(l{3pt}r{3pt}){3-5} \cmidrule(l{3pt}r{3pt}){6-8} \cmidrule(l{3pt}r{3pt}){9-11}
Component & Param. & True & MSN Est. (95\% CrI) & MN Est. (95\% CrI)  & True & MSN Est. (95\% CrI) & MN Est. (95\% CrI) & True & MSN Est. (95\% CrI) & MN Est. (95\% CrI)\\
\midrule
\addlinespace[0.3em]
\multicolumn{11}{l}{\textbf{ }}\\
\hspace{1em}MVSN & $\beta_{11}$ & 11 & 11.07 (10.74, 11.39) & 9.42 (8.91, 9.77) & -5 & -4.95 (-5.2, -4.68) & -4.11 (-4.33, -3.86) & -10 & -10.3 (-10.56, -10.01) & -8.37 (-8.86, -0.65)\\
\hspace{1em}Regression & $\beta_{21}$ & 12 & 12.02 (11.87, 12.17) & 11.98 (11.77, 12.18) & -4 & -4 (-4.1, -3.89) & -3.98 (-4.09, -3.87) & -11 & -11 (-11.19, -10.82) & -10.85 (-11.1, 0.61)\\
\hspace{1em} & $\beta_{31}$ & 13 & 13.06 (12.75, 13.36) & 11.39 (10.7, 11.78) & -3 & -2.97 (-3.25, -2.68) & -3.74 (-3.99, -3.5) & -12 & -11.9 (-12.22, -11.56) & -10.29 (-10.78, -0.14)\\
\hspace{1em} & $\beta_{41}$ & 14 & 14.06 (13.91, 14.22) & 14.02 (13.78, 14.22) & -2 & -1.96 (-2.07, -1.86) & -1.97 (-2.08, -1.84) & -13 & -13.04 (-13.25, -12.87) & -12.89 (-13.13, 0.59)\\
\hspace{1em} & $\beta_{12}$ & 2 & 2.11 (1.82, 2.35) & 0.42 (0.03, 0.83) & 5 & 5.16 (4.88, 5.47) & 5.83 (5.59, 6.07) & -2 & -1.86 (-2.21, -1.52) & -0.35 (-0.77, 0.16)\\
\hspace{1em} & $\beta_{22}$ & 2 & 2.03 (1.88, 2.17) & 2.02 (1.86, 2.22) & 5 & 4.96 (4.86, 5.06) & 4.96 (4.84, 5.07) & -2 & -1.97 (-2.18, -1.79) & -1.89 (-2.11, 0.02)\\
\hspace{1em} & $\beta_{32}$ & 2 & 2.13 (1.8, 2.43) & 0.49 (0.14, 0.86) & 5 & 5.22 (4.96, 5.49) & 4.23 (3.96, 4.5) & -2 & -1.82 (-2.14, -1.5) & -0.37 (-0.77, 0.16)\\
\hspace{1em} & $\beta_{42}$ & 2 & 2.08 (1.93, 2.23) & 2.08 (1.92, 2.28) & 5 & 4.97 (4.86, 5.08) & 4.96 (4.83, 5.08) & -2 & -1.93 (-2.13, -1.77) & -1.84 (-2.04, 0.04)\\
\addlinespace[0.3em]
\multicolumn{11}{l}{\textbf{ }}\\
\hspace{1em} & $\Omega_{11}$ & 5 & 4.99 (3.84, 6.52) & 3.08 (2.69, 4.16) & 2 & 1.95 (1.52, 2.52) & 1.38 (1.2, 1.59) & 5 & 6.27 (4.84, 7.88) & 3.49 (2.89, 186.69)\\
\hspace{1em} & $\Omega_{12}$ & 4.5 & 4.55 (3.49, 5.88) & 2.76 (2.39, 3.55) & -0.5 & -0.51 (-0.83, -0.22) & 0.2 (0.05, 0.36) & 4.5 & 4.95 (3.78, 6.43) & 3.02 (2.48, 215.36)\\
\hspace{1em} & $\Omega_{13}$ & 4.25 & 4.53 (3.48, 5.85) & 1.9 (1.58, 2.46) & 1.25 & 1.08 (0.74, 1.53) & 0.38 (0.24, 0.54) & 4.25 & 4.7 (3.56, 6.04) & 1.86 (1.42, 8.74)\\
\hspace{1em} & $\Omega_{14}$ & 4.12 & 4.33 (3.31, 5.57) & 1.78 (1.46, 2.24) & -0.88 & -1.08 (-1.5, -0.75) & -0.41 (-0.58, -0.26) & 4.12 & 4.51 (3.43, 5.69) & 1.72 (1.29, 12.25)\\
\hspace{1em} & $\Omega_{22}$ & 5 & 5.09 (3.98, 6.55) & 3.43 (3.01, 4.23) & 2 & 1.99 (1.53, 2.51) & 1.43 (1.22, 1.67) & 5 & 4.87 (3.75, 6.33) & 3.71 (3.09, 253.61)\\
\hspace{1em} & $\Omega_{23}$ & 4.5 & 4.77 (3.69, 6) & 2.13 (1.78, 2.7) & -0.5 & -0.46 (-0.75, -0.18) & 0.05 (-0.1, 0.2) & 4.5 & 4.29 (3.23, 5.61) & 2.02 (1.55, 13.33)\\
\hspace{1em} & $\Omega_{24}$ & 4.25 & 4.38 (3.39, 5.68) & 1.86 (1.53, 2.39) & 1.25 & 1.45 (1.04, 1.89) & 0.64 (0.48, 0.83) & 4.25 & 4.01 (3.04, 5.19) & 1.78 (1.35, 16.8)\\
\hspace{1em} & $\Omega_{33}$ & 5 & 5.47 (4.3, 6.67) & 2.55 (2.23, 3.2) & 2 & 1.73 (1.36, 2.3) & 1.41 (1.22, 1.64) & 5 & 4.79 (3.6, 6.24) & 2.44 (1.99, 5.81)\\
\hspace{1em} & $\Omega_{34}$ & 4.5 & 4.8 (3.7, 6.06) & 1.99 (1.69, 2.68) & -0.5 & -0.68 (-1.07, -0.37) & 0.14 (-0.01, 0.3) & 4.5 & 4.25 (3.2, 5.45) & 1.94 (1.55, 5.3)\\
\hspace{1em} & $\Omega_{44}$ & 5 & 5.17 (3.96, 6.69) & 2.45 (2.14, 3.79) & 2 & 2.34 (1.8, 2.97) & 1.57 (1.37, 1.83) & 5 & 4.64 (3.54, 5.92) & 2.37 (1.93, 5.68)\\
\addlinespace[0.3em]
\multicolumn{11}{l}{\textbf{ }}\\
\hspace{1em} & $\alpha_{1}$ & -0.99 & -0.81 (-2.12, 0.05) & / & 0.85 & 1.05 (0.37, 1.91) & / & 0.99 & 2.82 (1.21, 4.3) & /\\
\hspace{1em} & $\alpha_{2}$ & -0.5 & -0.22 (-1.3, 0.75) & / & -1.28 & -1.29 (-2.22, -0.5) & / & 0.5 & -0.07 (-1.14, 1.13) & /\\
\hspace{1em} & $\alpha_{3}$ & -0.5 & -0.96 (-2.14, 0.01) & / & 1.28 & 1.16 (0.47, 2.06) & / & 0.5 & 0.08 (-0.99, 1.46) & /\\
\hspace{1em} & $\alpha_{4}$ & -0.99 & -1.18 (-2.44, -0.06) & / & -0.85 & -0.91 (-1.76, -0.16) & / & 0.99 & 1.1 (0.07, 2.33) & /\\
\addlinespace[0.3em]
\multicolumn{11}{l}{\textbf{ }}\\
\hspace{1em}Multinom. & $\delta_{11}$ & -0.08 & -0.07 (-0.27, 0.12) & -0.54 (-0.77, -0.32) & -0.08 & -0.07 (-0.27, 0.12) & -0.54 (-0.77, -0.32) & -0.08 & -0.07 (-0.27, 0.12) & -0.54 (-0.77, -0.32)\\
\hspace{1em} & $\delta_{12}$ & 0.51 & 0.25 (-0.04, 0.53) & -0.26 (-0.6, 0.05) & 0.51 & 0.25 (-0.04, 0.53) & -0.26 (-0.6, 0.05) & 0.51 & 0.25 (-0.04, 0.53) & -0.26 (-0.6, 0.05)\\
\hspace{1em} & $\delta_{21}$ & -0.97 & -0.91 (-1.15, -0.68) & -0.07 (-0.28, 0.14) & -0.97 & -0.91 (-1.15, -0.68) & -0.07 (-0.28, 0.14) & -0.97 & -0.91 (-1.15, -0.68) & -0.07 (-0.28, 0.14)\\
\hspace{1em} & $\delta_{22}$ & 0.84 & 0.39 (0.09, 0.71) & 0.24 (-0.04, 0.5) & 0.84 & 0.39 (0.09, 0.71) & 0.24 (-0.04, 0.5) & 0.84 & 0.39 (0.09, 0.71) & 0.24 (-0.04, 0.5)\\
\addlinespace[0.3em]
\multicolumn{11}{l}{\textbf{ }}\\
\hspace{1em}Clustering & $\pi_l$ & 0.38 & 0.38 (0.38, 0.38) & 0.38 (0.13, 0.41) & 0.4 & 0.39 (0.39, 0.4) & 0.39 (0.36, 0.43) & 0.23 & 0.23 (0.22, 0.23) & 0.23 (0.2, 0.44)\\
\bottomrule
\end{tabular}
\end{table}
\end{landscape}

\begin{landscape}\begin{table}[t]

\caption{\label{tab:unnamed-chunk-5}Model results for simulated data with n = 1,000, J = 4, p = 2, K = 3, r = 2. 1,000 iterations were run with a burn in of 100. Missingness mechanism was MAR and P(miss) = 0. Model results for the multivariate skew normal (MSN) and multivariate normal (MN) mixtures are presented. This table compares the MSN to MVN when no skewness is present.}
\centering
\fontsize{6}{8}\selectfont
\begin{tabular}{lllllllllll}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{3}{c}{Class 1} & \multicolumn{3}{c}{Class 2} & \multicolumn{3}{c}{Class 3} \\
\cmidrule(l{3pt}r{3pt}){3-5} \cmidrule(l{3pt}r{3pt}){6-8} \cmidrule(l{3pt}r{3pt}){9-11}
Component & Param. & True & MSN Est. (95\% CrI) & MN Est. (95\% CrI)  & True & MSN Est. (95\% CrI) & MN Est. (95\% CrI) & True & MSN Est. (95\% CrI) & MN Est. (95\% CrI)\\
\midrule
\addlinespace[0.3em]
\multicolumn{11}{l}{\textbf{ }}\\
\hspace{1em}MVSN & $\beta_{11}$ & 11 & 11.23 (10.8, 11.7) & 10.92 (10.68, 11.16) & -5 & -4.93 (-5.44, -4.47) & -5 (-5.22, -4.76) & -10 & -9.72 (-10.72, -9.03) & -9.83 (-10.2, -9.42)\\
\hspace{1em}Regression & $\beta_{21}$ & 12 & 12.07 (11.96, 12.17) & 12.02 (11.88, 12.16) & -4 & -3.99 (-4.09, -3.9) & -3.99 (-4.09, -3.88) & -11 & -10.92 (-11.05, -10.77) & -10.84 (-11.05, -10.63)\\
\hspace{1em} & $\beta_{31}$ & 13 & 13.69 (13.16, 13.98) & 13 (12.73, 13.29) & -3 & -2.91 (-3.69, -2.31) & -2.91 (-3.12, -2.69) & -12 & -12.21 (-13.03, -11.11) & -11.89 (-12.32, -11.45)\\
\hspace{1em} & $\beta_{41}$ & 14 & 14.02 (13.92, 14.13) & 13.98 (13.81, 14.12) & -2 & -2 (-2.1, -1.9) & -2 (-2.1, -1.91) & -13 & -12.9 (-13.05, -12.74) & -12.8 (-13.05, -12.55)\\
\hspace{1em} & $\beta_{12}$ & 2 & 1.92 (1.57, 2.41) & 1.98 (1.77, 2.18) & 5 & 4.78 (4.24, 5.47) & 4.94 (4.71, 5.17) & -2 & -2.08 (-2.75, -1.22) & -1.94 (-2.25, -1.62)\\
\hspace{1em} & $\beta_{22}$ & 2 & 2.01 (1.9, 2.1) & 2 (1.9, 2.1) & 5 & 5 (4.9, 5.1) & 5 (4.88, 5.1) & -2 & -1.84 (-1.99, -1.69) & -1.83 (-2, -1.66)\\
\hspace{1em} & $\beta_{32}$ & 2 & 2.28 (1.7, 2.6) & 1.94 (1.74, 2.16) & 5 & 4.41 (4.03, 5.24) & 4.88 (4.67, 5.11) & -2 & -2.01 (-2.81, -1.43) & -1.98 (-2.28, -1.69)\\
\hspace{1em} & $\beta_{42}$ & 2 & 1.92 (1.82, 2.02) & 1.92 (1.82, 2.02) & 5 & 4.98 (4.88, 5.09) & 4.97 (4.86, 5.08) & -2 & -2 (-2.14, -1.87) & -2 (-2.14, -1.86)\\
\addlinespace[0.3em]
\multicolumn{11}{l}{\textbf{ }}\\
\hspace{1em} & $\Omega_{11}$ & 1 & 1.04 (0.86, 1.48) & 1.63 (1.42, 1.89) & 1 & 1 (0.83, 1.36) & 1.04 (0.89, 1.24) & 1 & 1.14 (0.84, 1.97) & 2.14 (1.77, 2.67)\\
\hspace{1em} & $\Omega_{12}$ & 0.5 & 0.6 (0.36, 1.04) & 1.24 (1.04, 1.49) & 0.5 & 0.5 (0.31, 0.87) & 0.49 (0.37, 0.62) & 0.5 & 0.67 (0.23, 1.56) & 2 (1.59, 2.6)\\
\hspace{1em} & $\Omega_{13}$ & 0.25 & 0.22 (0.09, 0.48) & 0.33 (0.2, 0.47) & 0.25 & 0.2 (-0.01, 0.4) & 0.08 (-0.03, 0.2) & 0.25 & 0.31 (-0.06, 0.89) & 0.53 (0.29, 0.82)\\
\hspace{1em} & $\Omega_{14}$ & 0.12 & 0.11 (-0.06, 0.4) & 0.17 (0.04, 0.31) & 0.12 & 0.07 (-0.21, 0.29) & -0.02 (-0.15, 0.09) & 0.12 & 0.15 (-0.19, 0.65) & 0.38 (0.18, 0.64)\\
\hspace{1em} & $\Omega_{22}$ & 1 & 1.42 (1.03, 1.95) & 1.95 (1.71, 2.23) & 1 & 1.13 (0.88, 1.64) & 0.99 (0.86, 1.16) & 1 & 1.44 (1.02, 2.37) & 2.9 (2.38, 3.63)\\
\hspace{1em} & $\Omega_{23}$ & 0.5 & 0.47 (0.27, 0.82) & 0.64 (0.5, 0.8) & 0.5 & 0.59 (0.38, 1.03) & 0.47 (0.34, 0.61) & 0.5 & 0.79 (0.47, 1.45) & 0.92 (0.61, 1.28)\\
\hspace{1em} & $\Omega_{24}$ & 0.25 & 0.42 (0.12, 0.77) & 0.38 (0.24, 0.54) & 0.25 & 0.18 (-0.11, 0.51) & 0.13 (0.01, 0.25) & 0.25 & 0.44 (0.11, 0.9) & 0.64 (0.4, 0.95)\\
\hspace{1em} & $\Omega_{33}$ & 1 & 1 (0.86, 1.25) & 1.01 (0.88, 1.16) & 1 & 1.19 (0.98, 1.69) & 1.19 (1.03, 1.4) & 1 & 1.32 (0.99, 1.96) & 1.2 (0.97, 1.49)\\
\hspace{1em} & $\Omega_{34}$ & 0.5 & 0.48 (0.32, 0.67) & 0.51 (0.41, 0.64) & 0.5 & 0.64 (0.35, 1.09) & 0.65 (0.52, 0.81) & 0.5 & 0.66 (0.41, 1.12) & 0.63 (0.47, 0.82)\\
\hspace{1em} & $\Omega_{44}$ & 1 & 1.07 (0.88, 1.42) & 0.99 (0.86, 1.14) & 1 & 1.32 (0.94, 1.9) & 1.12 (0.97, 1.34) & 1 & 1.06 (0.81, 1.6) & 1.01 (0.82, 1.23)\\
\addlinespace[0.3em]
\multicolumn{11}{l}{\textbf{ }}\\
\hspace{1em} & $\alpha_{1}$ & 0 & 0.05 (-0.89, 0.61) & 0 (0, 0) & 0 & -0.17 (-0.79, 0.65) & 0 (0, 0) & 0 & -0.55 (-1.8, 1.11) & 0 (0, 0)\\
\hspace{1em} & $\alpha_{2}$ & 0 & -1.24 (-2.02, 0.17) & 0 (0, 0) & 0 & -0.01 (-1.4, 1.36) & 0 (0, 0) & 0 & 0.22 (-1.09, 1.95) & 0 (0, 0)\\
\hspace{1em} & $\alpha_{3}$ & 0 & 0.96 (-0.41, 1.55) & 0 (0, 0) & 0 & 0.15 (-1.46, 1.04) & 0 (0, 0) & 0 & 0.11 (-1.13, 1.08) & 0 (0, 0)\\
\hspace{1em} & $\alpha_{4}$ & 0 & -0.67 (-1.41, 0.31) & 0 (0, 0) & 0 & 0.82 (-0.67, 1.82) & 0 (0, 0) & 0 & -0.12 (-0.91, 1.53) & 0 (0, 0)\\
\addlinespace[0.3em]
\multicolumn{11}{l}{\textbf{ }}\\
\hspace{1em}Multinom. & $\delta_{11}$ & -0.22 & -0.01 (-0.22, 0.18) & 0.09 (-0.12, 0.32) & -0.22 & -0.01 (-0.22, 0.18) & 0.09 (-0.12, 0.32) & -0.22 & -0.01 (-0.22, 0.18) & 0.09 (-0.12, 0.32)\\
\hspace{1em} & $\delta_{12}$ & -0.04 & -0.23 (-0.52, 0.03) & 0.79 (0.49, 1.08) & -0.04 & -0.23 (-0.52, 0.03) & 0.79 (0.49, 1.08) & -0.04 & -0.23 (-0.52, 0.03) & 0.79 (0.49, 1.08)\\
\hspace{1em} & $\delta_{21}$ & -0.67 & -0.57 (-0.8, -0.34) & 0.03 (-0.17, 0.26) & -0.67 & -0.57 (-0.8, -0.34) & 0.03 (-0.17, 0.26) & -0.67 & -0.57 (-0.8, -0.34) & 0.03 (-0.17, 0.26)\\
\hspace{1em} & $\delta_{22}$ & -0.24 & -0.35 (-0.67, -0.01) & -0.59 (-0.95, -0.21) & -0.24 & -0.35 (-0.67, -0.01) & -0.59 (-0.95, -0.21) & -0.24 & -0.35 (-0.67, -0.01) & -0.59 (-0.95, -0.21)\\
\addlinespace[0.3em]
\multicolumn{11}{l}{\textbf{ }}\\
\hspace{1em}Clustering & $\pi_l$ & 0.43 & 0.42 (0.41, 0.43) & 0.42 (0.39, 0.45) & 0.37 & 0.37 (0.36, 0.38) & 0.37 (0.34, 0.4) & 0.2 & 0.2 (0.2, 0.21) & 0.21 (0.18, 0.23)\\
\bottomrule
\end{tabular}
\end{table}
\end{landscape}


\newpage
\subsection*{Figures}

\begin{figure}[h]
	\label{fig:skew_resids}
	\caption{The distribution of residuals in repeated measures regression model of Bayley composite scores adjusted for race and sex.}
	\centering
	\includegraphics[width = 1\textwidth]{bayley_resids_plot}
\end{figure}

\begin{figure}[h]
	\label{fig:sim1_dens}
	\caption{Marginal densities of simulated outcomes in each cluster at each measurement occasion.}
	\centering
	\includegraphics[width = 1\textwidth]{y_densities.pdf}
\end{figure}	

\begin{figure}[h]
	\label{fig:sim1_means}
	\caption{Mean plots of simulated outcomes in each cluster at each measurement occasion.}
	\centering
	\includegraphics[width = 1\textwidth]{y_means.pdf}
\end{figure}

\begin{figure}
	\label{fig:sim1_beta_trace}
	\caption{Trace plots of draws from posterior distribution of $\beta$ parameters}
	\centering
	\includegraphics[width = 1\textwidth]{beta_trace.pdf}
\end{figure}	

\begin{figure}
	\label{fig:sim1_psi_trace}
	\caption{Trace plots of draws from posterior distribution of $\psi$ parameters}
	\centering
	\includegraphics[width = 1\textwidth]{psi_trace.pdf}
\end{figure}	

\begin{figure}
	\label{fig:sim1_sig2_trace}
	\caption{Trace plots of draws from posterior distribution of $\Sigma$ parameters}
	\centering
	\includegraphics[width = 1\textwidth]{sigma_trace.pdf}
\end{figure}	

\begin{figure}
	\label{fig:sim1_pi_trace}
	\caption{Trace plots of proportion within each class at each MCMC iteration}
	\centering
	\includegraphics[width = 1\textwidth]{pi_trace.pdf}
\end{figure}

\end{document}