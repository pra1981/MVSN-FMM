\documentclass{article}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}

\def\bSig\mathbf{\Sigma}
\newcommand{\VS}{V\&S}
\newcommand{\tr}{\mbox{tr}}

\begin{document}

\section{Model}
\label{s:model}

\subsection{Generic Multivariate Skew Normal Mixture}

Let $\mathbf{y}_{i}$ be the $J \times 1$ observation vector for subject $i$ such that $y_{ij}$ is the observation for subject $i$ at timepoint $j$. We assume for now that $\mathbf{y}_{i}$ is fully observed. Later, we relax this assumption by allowing for missingness in the components of $\mathbf{y}_{i}$. For the analysis of the Nurture data, we propose the following MSN mixture
\begin{equation}
f(\mathbf{y}_i) = \sum_{k = 1}^{K} \pi_{ik} f(\mathbf{y}_i|\boldsymbol\theta_k),
\end{equation}
where $\boldsymbol\theta_k$ is the set of parameters specific to cluster $k$ ($k = 1,...,K$). For now we assume that $K$ is fixed, but we discuss model selection strategies for choosing the optimal value of $K$ in Section 4. The probability that subject $i$ ($i = 1,...,n$) belongs to cluster $k$ is denoted $\pi_{ik}$. Note that $\pi_{ik}$ is indexed by $i$, as we allow the probability of class membership to vary across subjects. However, we assume a that class membership is fixed throughout the study period, since our focus is to cluster individuals based on their overall developmental patterns over the course of the study. In Section 6, we discuss extensions to allow for class membership to vary over time.

In each cluster, we assume that the $J\times 1$ outcome vector for subject $i$ follows an MSN distribution. Let $z_i \in \{1,...,K\}$ denote a latent cluster membership of subject $i$. Conditional on $z_i = k$, we model $\mathbf{y}_{i}$ as 
\begin{equation}
\mathbf{y}_{i}|z_i=k \sim MSN_J(\boldsymbol\zeta_k,\boldsymbol\alpha_k,\boldsymbol\Omega_k),
\end{equation}
where $\boldsymbol\zeta_k$ is a $J \times 1$ vector of cluster-specific location parameters, $\boldsymbol\alpha_k$ is a $J \times 1$ vector of cluster-specific skewness parameters, and $\boldsymbol\Omega_k$ is a $J \times J$ cluster-specific scale matrix that accounts for associations between the $J$ responses. The vector $\boldsymbol\alpha_k$ has components $\alpha_{jk}$, $j = 1,...,J$, that control the skewness of outcome $j$ in cluster $k$. When $\boldsymbol\alpha_k = \mathbf{0}$, the MSN distribution reduces to the multivariate normal distribution $N_J(\boldsymbol\zeta_k,\boldsymbol\Sigma_k)$, where $\boldsymbol\Sigma_J$ is the $J \times J$ covariance matrix capturing dependence among the $J$ outcomes. 

\subsection{Multivariate Skew Normal Stochastic Representation}

We model the effect of covariates on longitudinal motor development outcomes through the use of a MSN regression model. We adopt a standard stochastic representation of a MSN random variable by conditioning on a subject-specific latent truncated normal random effect, denoted $t_i$ (Fr\"{u}hwirth-Schnatter and Pyne 2010). Conditional on membership in cluster $k$, the stochastic representation for the $j^{th}$ component of the multivariate skew normal distribution for subject $i$ is 
\begin{equation}
y_{ij} = \zeta_{ijk} +  \psi_{jk} t_i + \sqrt{1-\psi^2_{jk}}\epsilon_{ijk}.
\end{equation}
where $y_{ij}$ denotes the $j^{th}$ observation for subject $i$, $\zeta_{ijk}$ denotes the location parameter for the $j^{th}$ observation for subject $i$ in cluster $k$, $t_i \stackrel{iid}{\sim} N_{[0,\infty)}(0,1)$ is a subject-specific truncated normal random effect, $\psi_{jk}$ controls the skewness of the $j^{th}$ outcome in cluster $k$, and $(\epsilon_{i1k},...,\epsilon_{iJk}) = \boldsymbol\epsilon_{ik} \sim N_J(0,\boldsymbol\Sigma_k)$ is a multivariate normal error term. Combining all observations for subject $i$ into $\mathbf{y}_i$, and conditional on $t_i$ and $z_i = k$, $\mathbf{y}_i$ follows a multivariate normal distribution with conditional mean $\mathbf{x}_i^T \boldsymbol\beta_k + t_i \boldsymbol\psi_k$ and conditional covariance $\boldsymbol\Sigma_k$. In the multivariate case, $\boldsymbol\psi_k$ is a $J \times 1$ vector containing cluster-specific skewness parameters. Marginally (after integrating over $t_i$), $\mathbf{y}_i$ is distributed $MSN_J(\boldsymbol\zeta_{ik}, \boldsymbol\alpha_k, \boldsymbol\Omega_k)$, where
$$\boldsymbol\zeta_{ik} = \mathbf{x}_i^T \boldsymbol\beta_k,$$
$$\boldsymbol\alpha_k = \frac{1}{\sqrt{1 - \boldsymbol\psi_k^T \boldsymbol\psi_k}} \boldsymbol\Omega^{-1}_k\boldsymbol\psi_k,$$
$$\boldsymbol\Omega_k = \boldsymbol\Psi_k \boldsymbol\Sigma_k \boldsymbol\Psi_k + \boldsymbol\psi_k \boldsymbol\psi_k^T,$$
where $\boldsymbol\Psi_k = \text{Diag}\left ( \sqrt{1 - \psi^2_{k1}},...,\sqrt{1 - \psi^2_{kJ}} \right )$. To allow for time varying effects in our model, we structure $\mathbf{X}_i$ as a $J \times pJ$ vector of covariate values for subject $i$, and $\boldsymbol\beta_k$ as a $pJ \times 1$ vector of fixed effects coefficients for cluster $k$ (think about how to explain this better). 

\subsection{Representation as Matrix Skew Normal}

We note that the multivariate skew normal density can be expressed more compactly in terms of the matrix skew normal (MatSN) density (Chen and Gupta 2005). As we will see in Section 3.6, this specification facilitates prior specification by admitting convenient joint prior distributions for the regression coefficients and scale matrices. In particular, we define $\mathbf{Y}_k$ as the ${n_k \times J}$  response matrix with rows $\mathbf{y}_i^T$, $(i = 1,...,n_k)$, where $n_k = \sum_{i = 1}^n {1}_{(z_i = k)}$ is the total number of observations belonging to cluster $k$ ($k = 1,...,K$). 
$$\mathbf{Y}_{k} \sim MatSN_{n_k \times J}(\mathbf{M},\boldsymbol\alpha_k,\mathbf{I}_{n_k},\boldsymbol\Omega_k)$$
$$\text{vec}(\mathbf{Y}) =  (\mathbf{y}^T_1,...,\mathbf{y}^T_{n_k})$$
$$\text{vec}(\mathbf{M}) = (\boldsymbol\zeta_1^T,...,\boldsymbol\zeta_{n_k}^T)$$
where $\boldsymbol\zeta_i = \mathbf{x}_i^T \boldsymbol\beta_k$ and $\boldsymbol\alpha_k = (\alpha_{k1},...,\alpha_{kJ})$. This implies that the $i^{th}$ row of $\mathbf{Y}_k$, a ${n_k \times J}$ matrix of responses for cluster $k$, is $\mathbf{y}_i \sim MSN_J(\boldsymbol\zeta_i, \boldsymbol\alpha_k, \boldsymbol\Omega_k)$. Using the stochastic representation presented in Section 3.2, the conditional distribution of $\mathbf{y}_i | z_i, t_i$ is $N_J (\mathbf{x}_i \boldsymbol\beta_k + \boldsymbol\psi_k t_i,\boldsymbol\Sigma_k)$. This implies that all responses for cluster $k$ conditioned on $\mathbf{t}_{k}$ is distributed as 
$$\mathbf{Y}_k | \mathbf{t}_k \sim MatNorm_{n_k \times J}(\mathbf{M}_k, \mathbf{I}_{n_k}, \boldsymbol\Sigma_k),$$
where $\text{vec}(\mathbf{M}_k)_{n_kJ \times 1} = \mathbf{X}_{k} \boldsymbol\beta_k + \mathbf{t}_k \otimes \boldsymbol\psi_k$ and $\mathbf{X}_k$ is an $n_k \times p$ design matrix.

\subsection{Multinomial Regression on Cluster Probabilities}

A primary concern of our model is with identification of latent infant development classs. We accomplish this via multinomial logit regression model on cluster membership, which utilizes P\'{o}lya-Gamma data-augmentation, as described in Section 3.6 to allow for updating of all parameters using Gibbs sampling. The multinomial logit model is as follows for $k = 1,...,K$.
$$P(z_i = k|\mathbf{w}_i) = \pi_{ik} = \frac{e^{\mathbf{w}_i^T \boldsymbol\delta_k}}{\sum_{k' = 1}^K e^{\mathbf{w}_i^T \boldsymbol\delta_{k'}}}$$



where $\mathbf{w}_i$ is the vector of cluster probability covariates for subject $i$, $\boldsymbol\delta_k$ contains the multinomial regression parameters for cluster $k$, and $K$ is the number of putative classs specified \textit{a priori}. For identifiability purposes, we fix the reference category $k = K$ and set $\boldsymbol\delta_K = \mathbf{0}$. Under this model, $z_i|\mathbf{w}_i \sim Multinom(1,\boldsymbol\pi_i)$, where $\boldsymbol\pi_i = (\pi_{i1},...,\pi_{1K})$. During MCMC estimation, the cluster labels $z_i$ are updated from their multinomial full conditional distribution and used in the remaining MCMC steps as cluster assignments.

\subsection{Conditional MVN Imputation}

We allow for missingness of outcomes in the MSN mixture model by imputing missing values from their conditional multivariate normal distributions. We note that
$$\mathbf{y}_i|z_i,\mathbf{x}_i,t_i,\boldsymbol\beta_{k},\boldsymbol\psi_{k},\boldsymbol\Sigma_{k} \sim N_J(\boldsymbol\beta_{k}^T\mathbf{x}_i  + t_i \boldsymbol\psi_{k}, \boldsymbol\Sigma_{k})$$

where $z_i = k$. This allows us to appeal to standard conditional forms of the multivariate normal distribution to specify the distribution of missing observations. For subject $i$, we use $q_i$ to denote the number of the total $J$ possible repeated measurements are missing. We partition the full $J \times 1$ outcome vector $\mathbf{y}_i$ into the $(J - q_i) \times 1$ observed data vector $\mathbf{y}_i^{obs}$ and the $q_i \times 1$ missing data vector $\mathbf{y}_i^{miss}$. $Y_i = [Y^{miss}_{i_{q \times 1}} | Y^{obs}_{i_{J - q \times 1}}]^T$. We have
$$\mathbf{y}_i^{miss}|\mathbf{y}_i^{obs},\mathbf{x}_i,t_i,z_i,\boldsymbol\beta_{k},\boldsymbol\psi_{k},\boldsymbol\Sigma_{k} \sim N(\boldsymbol\mu^{miss},\boldsymbol\Sigma^{miss})$$

where $\boldsymbol\mu^{miss}$ and $\boldsymbol\Sigma^{miss}$ take standard forms detailed in the Appendix. 

An attractive feature of this imputation approach is that each missing outcome is imputed ``online", i.e. once per MCMC iteration. This provides more opportunities to explore the parameter space than multiple imputation and avoids multiplicative run-time scaling in $m$, the number of imputations. We demonstrate this feature using simulations detailed in Section 4. 

\subsection{Bayesian Inference}

\subsubsection{P\'olya--Gamma Data Augmentation}

Polson \textit{et al.} (2013) introduced an efficient data augmentation approach to fitting several GLMs including the multinomial logit regression model specified by Holmes and Held (2006), which specifies the full conditional distributions of the multinomial regression parameters as a function of a Bernoulli likelihood. 
$$p(\boldsymbol\delta_k|\mathbf{z},\boldsymbol\delta_{k' \ne k}) \propto p(\boldsymbol\delta_k) \prod_{i = 1}^{n} \pi_{ik}^{U_{ik}}(1-\pi_{ik})^{1-U_{ik}}$$

where $p(\boldsymbol\delta_k)$ denotes the prior distribution of $\boldsymbol\delta_k$, $U_{ik} = {1}_{z_i = k}$ is an indicator that subject $i$ belongs to cluster $k$, and $\pi_{ik}$ is defined as in Section 3.4. We can rewrite $\pi_{ik}$ as follows
$$\pi_{ik} = P(U_{ik} = 1) = \frac{e^{\mathbf{w}_i^T \boldsymbol\delta_k - {c}_{ik}}}{1 + e^{\mathbf{w}_i^T \boldsymbol\delta_k - {c}_{ik}}} = \frac{e^{\eta_{ik}}}{1 + e^{\eta_{ik}}}$$

where ${c}_{ik} = \log \sum_{k' \ne k} e^{\mathbf{w}_i^T \boldsymbol\delta_{k'}}$ and $\eta_{ik} = \mathbf{w}_i^T \boldsymbol\delta_k - {c}_{ik}$. We note that the sum $\sum_{k' \ne k} e^{\mathbf{w}_i^T \boldsymbol\delta_{k'}}$ includes the reference category, but since we fix $\boldsymbol\delta_K = \mathbf{0}$, we have $e^{\mathbf{w}_i^T \boldsymbol\delta_K} = 1$, and hence
$$c_{ik} = \log \sum_{k' \ne k} e^{\mathbf{w}_i^T \boldsymbol\delta_{k'}} = \log \left ( 1 + \sum_{k' \notin \{k,K \}} e^{\mathbf{w}_i^T \boldsymbol\delta_{k'}} \right )$$

We can use the quantities to re-express the full conditionals for $\boldsymbol\delta_k$ as 
$$p(\boldsymbol\delta_k|\mathbf{Z},\boldsymbol\delta_{k' \ne k}) \propto p(\boldsymbol\delta_k) \prod_{i = 1}^{n} \left (\frac{e^{\eta_{ik}}}{1 + e^{\eta_{ik}}} \right )^{U_{ik}} \left (\frac{1}{1 + e^{\eta_{ik}}} \right )^{1-U_{ik}} = p(\boldsymbol\delta_k) \prod_{i = 1}^n \frac{(e^{\eta_{ik}})^{U_{ik}}}{1 + e^{\eta_{ik}}}$$

which we note is essentially a logistic regression likelihood. We thus apply this P\'olya--Gamma data augmentation scheme to update each $\boldsymbol\delta_k$ ($k = 1,...,K-1$) one at a time based on the binary indicators $U_{ik}$.

\begin{itemize}

\item Emphasize that PG data augmentation for the multinomial model results in a PG mixture of experts model, which is a computationally efficient way to model edge weights. 

\end{itemize}

\subsubsection{Prior Choice}

\subsubsection{MCMC Algorithm}

\begin{algorithm}
\caption{Gibbs Sampler}
\begin{algorithmic}
\small
\linespread{0.5}
    \State $\mathbf{Define} \ n_{iter};\  n_{burn}; \ K; \ \boldsymbol\theta_{init}; \ \boldsymbol\theta_0$
    \State $n_{sim} := n_{iter} - n_{burn}$
    \State $\boldsymbol\theta := \boldsymbol\theta_{init}$
    \For {$\iota = 1,...,n_{sim}$}
        \State \textsc{I. Conditional Imputation}
          \For {$ i = 1,...,n$}
            \State $\mathbf{Draw} \ \mathbf{y}_i^{miss} \ \text{from} \ N_q(\boldsymbol\mu_i^{miss}, \boldsymbol\Sigma_i^{miss})$
            \State $\mathbf{y}_i := \mathbf{y}_i^{miss} \cup \mathbf{y}_i^{obs}$
         \EndFor
        \State \textsc{II. MSN Regression}
          \For {$ k = 1,...,K$}
            \State $n_k := \sum_{i = 1}^n {1}_{z_i = k}$
            \For {$i_k = 1,...,n_k$}
              \State $\mathbf{Draw}\  t_i \ \text{from} \ N_{[0,\infty)}(a_i,A)$
            \EndFor
            \State $\mathbf{X^*}_k := $ \texttt{cbind}$(\mathbf{X}_k,\mathbf{t}_k)$
            \State $\mathbf{Draw} \ \mathbf{B^*}_k \ \text{from} \ \text{MatNorm}(\mathbf{B}_k,\mathbf{L}_k^{-1},\boldsymbol\Sigma_k)$
            \State $\mathbf{Draw} \ \boldsymbol\Sigma_k \ \text{from} \ \text{InvWish}(\nu_k, \mathbf{V}_k)$
          \EndFor
        \State \textsc{III. Multinomial Logit}
          \For {$i = 1,...,n$}
            \For {$k = 1,...,K$}
              \State $\pi_{ik} := P(z_i = k|\mathbf{w}_i,\boldsymbol\delta_k)$
              \State $p_{ik} := P(\mathbf{y}_i|\boldsymbol\beta_k^{*T} \mathbf{x}^*_i,\boldsymbol\Sigma_k)$
            \EndFor
            \State $\mathbf{p}_{z_i} := \frac{\mathbf{p}_i \circ \boldsymbol\pi_i}{\mathbf{p}_i \cdot \boldsymbol\pi_i}$
            \State $\mathbf{Draw} \ z_i \ \text{from} \ \text{Categorical}(\mathbf{p}_{z_i})$
            \For {$k = 1,...,K-1$}
              \State $\mathbf{Draw} \ \boldsymbol\delta_k \ \text{from} \ N(\mathbf{M},\mathbf{S})$
            \EndFor
          \EndFor
        \State $\boldsymbol\theta := \{\mathbf{B^*}, \boldsymbol\Sigma, \mathbf{Z}, \boldsymbol\delta \}$
        \State $\mathbf{Store} \ \boldsymbol\theta$
	  \EndFor
\end{algorithmic}
\end{algorithm}


\subsubsection{Assessment of MCMC Convergence}

\subsubsection{Label Switching}


\end{document}