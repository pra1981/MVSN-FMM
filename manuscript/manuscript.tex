%  ALWAYS USE THE referee OPTION WITH PAPERS SUBMITTED TO BIOMETRICS!!!
%  You can see what your paper would look like typeset by removing
%  the referee option.  Because the typeset version will be in two
%  columns, however, some of your equations may be too long. DO NOT
%  use the \longequation option discussed in the user guide!!!  This option
%  is reserved ONLY for equations that are impossible to split across 
%  multiple lines; e.g., a very wide matrix.  Instead, type your equations 
%  so that they stay in one column and are split across several lines, 
%  as are almost all equations in the journal.  Use a recent version of the
%  journal as a guide. 
%  
\documentclass[useAMS,referee]{biom}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{blindtext}
\usepackage{booktabs}
\usepackage{breqn}
\usepackage{algorithm}
\usepackage{algcompatible}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{dsfont}


\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}


\def\bSig\mathbf{\Sigma}
\newcommand{\VS}{V\&S}
\newcommand{\tr}{\mbox{tr}}

%  Here, place your title and author information.  Note that in 
%  use of the \author command, you create your own footnotes.  Follow
%  the examples below in creating your author and affiliation information.
%  Also consult a recent issue of the journal for examples of formatting.

\title[]{Bayesian multivariate skew-normal finite mixture model for analysis of infant development trajectories}

%  Here are examples of different configurations of author/affiliation
%  displays.  According to the Biometrics style, in some instances,
%  the convention is to have superscript *, **, etc footnotes to indicate 
%  which of multiple email addresses belong to which author.  In this case,
%  use the \email{ } command to produce the emails in the display.

%  In other cases, such as a single author or two authors from 
%  different institutions, there should be no footnoting.  Here, use
%  the \emailx{ } command instead. 

%  The examples below corrspond to almost every possible configuration
%  of authors and may be used as a guide.  For other configurations, consult
%  a recent issue of the the journal.

%  Single author -- USE \emailx{ } here so that no asterisk footnoting
%  for the email address will be produced.

%\author{John Author\emailx{email@address.edu} \\
%Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K.}

%  Two authors from the same institution, with both emails -- use
%  \email{ } here to produce the asterisk footnoting for each email address

%\author{John Author$^{*}$\email{author@address.edu} and
%Kathy Authoress$^{**}$\email{email2@address.edu} \\
%Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K.}

%  Exactly two authors from different institutions, with both emails  
%  USE \emailx{ } here so that no asterisk footnoting for the email address
%  is produced.

\author
{Carter Allen\emailx{allecart@musc.edu} \\
Department of Public Health Sciences, Medical University of South Carolina, Charleston, SC, U.S.A
\and
Brian Neelon, PhD \\
Department of Public Health Sciences, Medical University of South Carolina, Charleston, SC, U.S.A
\and
Sara Benjamin-Neelon, PhD, MPH, RD \\
Department of Health, Behavior and Society, Johns Hopkins University, Baltimore, MD, U.S.A}

%  Three or more authors from same institution with all emails displayed
%  and footnoted using asterisks -- use \email{ } 

%\author{John Author$^*$\email{author@address.edu}, 
%Jane Author$^{**}$\email{jane@address.edu}, and 
%Dick Author$^{***}$\email{dick@address.edu} \\
%Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K}

%  Three or more authors from same institution with one corresponding email
%  displayed

%\author{John Author$^*$\email{author@address.edu}, 
%Jane Author, and Dick Author \\
%Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K}

%  Three or more authors, with at least two different institutions,
%  more than one email displayed 

%\author{John Author$^{1,*}$\email{author@address.edu}, 
%Kathy Author$^{2,**}$\email{anotherauthor@address.edu}, and 
%Wilma Flinstone$^{3,***}$\email{wilma@bedrock.edu} \\
%$^{1}$Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K \\
%$^{2}$Department of Biostatistics, University of North Carolina at 
%Chapel Hill, Chapel Hill, North Carolina, U.S.A. \\
%$^{3}$Department of Geology, University of Bedrock, Bedrock, Kansas, U.S.A.}

%  Three or more authors with at least two different institutions and only
%  one email displayed

%\author{John Author$^{1,*}$\email{author@address.edu}, 
%Wilma Flinstone$^{2}$, and Barney Rubble$^{2}$ \\
%$^{1}$Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K \\
%$^{2}$Department of Geology, University of Bedrock, Bedrock, Kansas, U.S.A.}


\begin{document}

%  This will produce the submission and review information that appears
%  right after the reference section.  Of course, it will be unknown when
%  you submit your paper, so you can either leave this out or put in 
%  sample dates (these will have no effect on the fate of your paper in the
%  review process!)

\date{{\it Received October} 2007. {\it Revised February} 2008.  {\it
Accepted March} 2008.}

%  These options will count the number of pages and provide volume
%  and date information in the upper left hand corner of the top of the 
%  first page as in published papers.  The \pagerange command will only
%  work if you place the command \label{firstpage} near the beginning
%  of the document and \label{lastpage} at the end of the document, as we
%  have done in this template.

%  Again, putting a volume number and date is for your own amusement and
%  has no bearing on what actually happens to your paper!  

\pagerange{\pageref{firstpage}--\pageref{lastpage}} 
\volume{64}
\pubyear{2008}
\artmonth{December}

%  The \doi command is where the DOI for your paper would be placed should it
%  be published.  Again, if you make one up and stick it here, it means 
%  nothing!

\doi{10.1111/j.1541-0420.2005.00454.x}

%  This label and the label ``lastpage'' are used by the \pagerange
%  command above to give the page range for the article.  You may have 
%  to process the document twice to get this to match up with what you 
%  expect.  When using the referee option, this will not count the pages
%  with tables and figures.  

\label{firstpage}

%  put the summary for your paper here

\begin{abstract}
In studies of infant motor development, a crucial research goal is to identify latent clusters of infants that experience delayed development, as this is a known risk factor for adverse outcomes later in life. However, there are a number of statistical challenges in modeling infant development: the data are typically skewed, exhibit intermittent missingness, and are highly correlated across the repeated measurements collected during infancy. Using data from the Nurture study, a cohort of over 600 mother-infant pairs followed from pregnancy to 12 months postpartum, we develop a flexible Bayesian latent class model for the analysis infant motor development. Our model has a number of attractive features. First, we adopt the multivariate skew normal distribution with cluster-specific parameters that accommodate the inherent correlation and skewness in the data. Second, we model the cluster membership probabilities using a novel Pólya-Gamma data-augmentation scheme, thereby improving predictions of the cluster membership allocations. Lastly, we impute missing responses under missing at random assumption by drawing from appropriate conditional skew normal distributions. Bayesian inference is achieved through straightforward Gibbs sampling, and can be carried out in available software such as R.  Through simulation studies, we show that the proposed model yields improved inferences over models that ignore skewness. In addition, our imputation method yields improvements compared to conventional missing data methods, including multiple imputation and complete or available case analysis. When applied to Nurture data, we identified two distinct development clusters: one characterized by delayed “U-shaped” development and a higher percentage of male infants and another characterized by more steady development and a lower percentage of males. The clusters also differed in terms of key demographic variables, such as infant race and maternal pre-pregnancy body mass index. These findings can aid investigators in targeting interventions during this critical early-life developmental window.
\end{abstract}

%  Please place your key words in alphabetical order, separated
%  by semicolons, with the first letter of the first word capitalized,
%  and a period at the end of the list.
%

\begin{keywords}
A key word; But another key word; Still another key word; Yet another key word.
\end{keywords}

%  As usual, the \maketitle command creates the title and author/affiliations
%  display

\maketitle

\setcounter{tocdepth}{3}
\tableofcontents

\newpage

%  If you are using the referee option, a new page, numbered page 1, will
%  start after the summary and keywords.  The page numbers thus count the
%  number of pages of your manuscript in the preferred submission style.
%  Remember, ``Normally, regular papers exceeding 25 pages and Reader Reaction 
%  papers exceeding 12 pages in (the preferred style) will be returned to 
%  the authors without review. The page limit includes acknowledgements, 
%  references, and appendices, but not tables and figures. The page count does 
%  not include the title page and abstract. A maximum of six (6) tables or 
%  figures combined is often required.''

%  You may now place the substance of your manuscript here.  Please use
%  the \section, \subsection, etc commands as described in the user guide.
%  Please use \label and \ref commands to cross-reference sections, equations,
%  tables, figures, etc.
%
%  Please DO NOT attempt to reformat the style of equation numbering!
%  For that matter, please do not attempt to redefine anything!

\section{Introduction}
\label{s:intro}

\subsection{Infant Development Clustering}

Heterogeneity of treatment effects (HTE) (Lanza and Rhoades, 2013). 

\subsection{Existing Approaches}

Mixtures of multivariate non-symmetric distributions such as the multivariate skew-normal (MSN) distribution allow for the nuances of the marginal density to be captured with a more parsimonious set of mixture components. Mixtures of MSN distributions have been dealt with previously in a Bayesian context (Fr\"{u}hwirth-Schnatter \& Pyne, 2010), however in these models, focus lies primary on marginal density estimation and inference on the mixture components (i.e. clusters) is not discussed. More recently, the mixtures of skew-$t$ factor analysis (MSTFA) model has been proposed for settings in which cluster-specific inference is of primary interest (Lin \textit{et al.} 2018). However, an important feature not included in the MSTFA is the ability to explain individual-level cluster membership as a function of covariates of interest. Additionally, parameter estimation proposed by Lin et al. for the MSTFA relies on a prohibitively complex EM algorithm and does not enjoy the inferential benefits of a Bayesian approach, namely the ability to incorporate prior information into a model and make posterior probability statements. Our proposed model improves on these previous works by estimating parameters in a Bayesian framework as well as including the ability to fit a multinomial logit regression to cluster membership probabilities using a novel application of data augmentation with the P\'olya Gamma distribution.

\textbf{\textit{Put lit review of Bayesian PG multinomial logistic regression here}}

A ubiquitous feature of repeated measures studies is loss of data due to intermittent missingness and attrition. In the Bayesian setting, the standard approach to dealing with missing data is to perform multiple imputation, whereby $m$ imputed data sets are generated from a specified imputation model. After $m$ complete data sets are obtained, parameter estimates are combined across each data set to produce a final set of parameter estimates (Gelman \textit{et al.} 2013). This approach is not only computationally burdensome, requiring storage and analysis of an $m \times n_{rows} \times n_{cols}$ data array in addition to multiplication of total model run time by a factor of $m$, but it has been shown to produce unreliable inferences (Zhou and Reiter, 2010). We instead include an ``online" imputation imputation step in our Gibbs sampling procedure, whereby missing outcomes are updated at each iteration. This approach greatly increases the number of opportunities for exploration of the missing data parameter space.

\newpage

\section{Nurture Study}
\label{s:nurt}
\subsection{Baseline Demographics and Description of Variables}

\subsection{Statistical Challenges}
\subsubsection{Skewness of Bayley score residuals}
\subsubsection{Attrition and Intermittent Missingness}

\newpage

\section{Model}
\label{s:model}

\subsection{Multivariate Skew Normal Mixture Model}

A primary goal of the Nurture study is to identify clusters of infants characterized by distinct motor development trajectories. To address this aim, we propose a flexible finite mixture model that accommodates relevant features of the data, such as skewness and dependence among the responses. To this end, let $\mathbf{y}_{i}=(y_{i1},\ldots,y_{iJ})^T$ be a $J \times 1$ vector of responses (i.e., Baley scores) for subject $i$ $(i=1,\ldots,n)$. For the analysis of the Nurture data, we propose a finite mixture model of the form
\begin{equation}
f(\mathbf{y}_i) = \sum_{k = 1}^{K} \pi_{ik} f(\mathbf{y}_i|\boldsymbol\theta_k),
\end{equation}
where $\boldsymbol\theta_k$ is the set of parameters specific to cluster $k$ ($k = 1,...,K$) and $\pi_{ik}$ is a subject-specific mixing weight representing the probability that subject $i$ belongs to cluster $k$. For now we assume that $K$ is fixed; in Section 4, we discuss model selection strategies for choosing the optimal value of $K$. 

To facilitate posterior inference, we introduce a latent cluster indicator variable $z_i$ taking the value $k \in \{1,...,K\}$ with probability $\pi_{ik}$. Conditional on $z_i = k$, we assume $\mathbf{y}_{i}$ is distributed as
\begin{equation}
\mathbf{y}_{i}|(z_i=k) \sim MSN_J(\boldsymbol\zeta_{ki},\boldsymbol\alpha_k,\boldsymbol\Omega_k), \label{eq:msndens}
\end{equation}
where $MSN_J(\cdot)$ denotes the $J$-dimensional multivariate skew normal density, $\boldsymbol\zeta_{ki}$ is a $J \times 1$ vector of subject- and cluster-specific location parameters, $\boldsymbol\alpha_k$ is a $J \times 1$ vector of cluster-specific skewness parameters, and $\boldsymbol\Omega_k$ is a $J \times J$ cluster-specific scale matrix that captures dependence among the $J$ responses. The vector $\boldsymbol\alpha_k$ has components $\alpha_{kj}$, $j = 1,...,J$, that control the skewness of outcome $j$ in cluster $k$. When $\boldsymbol\alpha_k = \mathbf{0}$, the MSN distribution reduces to the multivariate normal distribution $N_J(\boldsymbol\zeta_k,\boldsymbol\Omega_k)$, where $\boldsymbol\Omega_k$ is a $J \times J$ covariance matrix.

We can extend model (\ref{eq:msndens}) to the regression setting by modeling $\boldsymbol\zeta_{ki}$ as a function of covariates. Here we adopt a convenient stochastic representation of the MSN density (Azzalini and Dalla Valle, 1996):
\begin{equation}
\mathbf{y}_{i}|(z_i=k,t_i) = \mathbf{X}_i \boldsymbol\beta_k + t_i \boldsymbol\psi_k + \boldsymbol\epsilon_{ki}, \label{eq:msnreg}
\end{equation}
where 
$\mathbf{X}_i$ is a $J \times Jp$ design matrix that includes potential time-varying covariates (e.g., indicators denoting quarterly visits); $\boldsymbol{\beta}_k=(\beta_{k11},\ldots,\beta_{k1p},\ldots,\beta_{kJ1},\ldots,\beta_{kJp})^T$ is a $Jp\times 1$ vector of cluster- and outcome-specific regression coefficients; $t_i\sim N_{[0,\infty)}(0,1)$ is a subject-specific standard normal random variable truncated below by zero; $\boldsymbol\psi_k=(\psi_{k1},\ldots,\psi_{kJ})^T$ is a $J \times 1$ vector of cluster-specific skewness parameters; and $\boldsymbol{\epsilon}_{ki} \sim N_J(\boldsymbol0,\boldsymbol\Sigma_k)$ is a $J\times 1$ vector of error terms. Thus, conditional on $t_i$ and $z_i=k$, $\boldsymbol{y}_i$ is distributed as $N_J(\mathbf{X}_i \boldsymbol\beta_k + t_i \boldsymbol\psi_k, \boldsymbol{\Sigma}_k)$. Marginally (after integrating over $t_i$), $\mathbf{y}_i$ is distributed $MSN_J(\boldsymbol\zeta_{ki}, \boldsymbol\alpha_k, \boldsymbol\Omega_k)$, where through back-transformation 
\begin{eqnarray*}
\boldsymbol\zeta_{ki} &=& \mathbf{X}_i\boldsymbol\beta_k,\\
\boldsymbol\alpha_k &=& \frac{1}{\sqrt{1 - \boldsymbol\psi_k^T 
\boldsymbol\Omega^{-1}_k\boldsymbol\psi_k}} \boldsymbol\omega_k \boldsymbol\Omega^{-1}_k\boldsymbol\psi_k,~~\text{and}\\
\boldsymbol\Omega_k &=& \boldsymbol\Sigma_k  + \boldsymbol\psi_k \boldsymbol\psi_k^T,
\end{eqnarray*}
where $\boldsymbol\omega_k = \text{Diag}(\sqrt{\omega_{k,11}},...,\sqrt{\omega_{k,JJ}})$ is the $J \times J$ diagonal matrix containing the square root of the diagonal entries of $\boldsymbol\Omega_k$. Additional details can be found in Fr\"{u}wirth-Schnatter and Pyne (2010). 

Of note, the MSN density can be expressed more compactly in terms of the matrix skew normal (MatSN) density (Chen and Gupta 2005). As we will see in Section 3.6, the matrix representation of the MSN distribution admits convenient conjugate prior distributions for the regression parameters and scale matrices, which in turn leads to efficient Gibbs sampling for posterior inference. Let $\mathbf{Y}_k$ be an ${n_k \times J}$ response matrix with rows $\mathbf{y}_i^T$, $(i = 1,...,n_k)$, where $n_k = \sum_{i = 1}^n {1}_{(z_i = k)}$ is the number of observations in cluster $k$. From equation (\ref{eq:msnreg}), it follows that $\mathbf{Y}_k$ is distributed as
\begin{eqnarray*}
\mathbf{Y}_{k} &\sim& MatSN_{n_k \times J}(\mathbf{M}_k,\boldsymbol\alpha_k,\mathbf{I}_{n_k},\boldsymbol\Omega_k)\\
%\text{vec}(\mathbf{Y}_k) =  (\mathbf{y}^T_1,...,\mathbf{y}^T_{n_k})^T\\
\text{vec}(\mathbf{M}_k) &=& (\boldsymbol\zeta_{k1}^T,...,\boldsymbol\zeta_{kn_k}^T)^T,
\end{eqnarray*}
where $\boldsymbol\zeta_{ki} = \mathbf{X}_i \boldsymbol\beta_k$ as in equation (\ref{eq:msnreg}), $\boldsymbol\alpha_k = (\alpha_{k1},...,\alpha_{kJ})^T$, $\mathbf{I}_{n_k}$ is the $n_k\times n_k$ identity matrix, and $\boldsymbol\Omega_k$ is the $J\times J$ scale matrix defined above in equation (\ref{eq:msndens}). From equation (\ref{eq:msnreg}), it follows that $\mathbf{Y}_{k}$, conditional on the $n_k\times 1$ vector of random effects $\mathbf{t}_{k}$, is jointly distributed in matrix form as
\[
\mathbf{Y}_k | \mathbf{t}_k \sim MatNorm_{n_k \times J}(\mathbf{M}_k, \mathbf{I}_{n_k}, \boldsymbol\Sigma_k),
\]
where $MatNorm_{n_k \times J}(\cdot)$ denotes a $n_k\times J$ matrix normal density, $\text{vec}(\mathbf{M}_k) = \mathbf{X}_{k} \boldsymbol\beta_k + \mathbf{t}_k \otimes \boldsymbol\psi_k$ is an $n_kJ\times 1$ mean vector, $\mathbf{X}_k$ is an $n_kJ \times Jp$ design matrix, $\boldsymbol\beta_k$ is the $(Jp)\times 1$ vector of regression coefficients defined in equation (\ref{eq:msnreg}), and $\boldsymbol\Sigma_k$ is the $J\times J$ conditional covariance of $\boldsymbol\epsilon_{ik}$ given in equation (\ref{eq:msnreg}).

\subsection{Multinomial Regression for the Cluster Indicators}
To accommodate heterogeneity in the cluster-membership probabilities, we model $\pi_{ik}$ as a function of coovariates using a multinomial logit model 
\begin{eqnarray}
\pi_{ik} = \Pr(z_i = k|\mathbf{w}_i) = \frac{e^{\mathbf{w}_i^T \boldsymbol\delta_k}}{\sum_{h = 1}^K e^{\mathbf{w}_i^T \boldsymbol\delta_{h}}},~ k=1,\ldots,K,
\end{eqnarray}
where $\mathbf{w}_i$ is an $r\times 1$ vector of subject-level covariates, $\boldsymbol\delta_k$ is a $r\times 1$ vector of regression parameters associated with membership in cluster $k$. For identifiability purposes, we fix the reference category $k = K$ and set $\boldsymbol\delta_K = \mathbf{0}$. Under this model, $z_i|\mathbf{w}_i \sim Multinom(1,\boldsymbol\pi_i)$, where $\boldsymbol\pi_i = (\pi_{i1},...,\pi_{1K})$. During MCMC estimation, the cluster labels $z_i$ are updated from their multinomial full conditional distribution and used in the remaining MCMC steps as cluster assignments.

\subsection{Conditional MSN Imputation}
To accommodate missing at random (MAR) responses, we propose a convenient imputation algorithm that can be implemented ``online'' as part of the Gibbs sampler. In Section 6, we discuss extensions to allow for non-ignorable missingness. Suppose $\mathbf{y}_i$ has $q_i\in (1,\ldots,J)$ observed values, denoted $\mathbf{y}^{obs}_i$, and $J-q_i$ intermittent missing values, denoted $\mathbf{y}^{miss}_i$. We can make use of the stochastic representation given in equation (\ref{eq:msnreg}) to impute $\mathbf{y}^{miss}_i$ from its conditional multivariate normal distribution given $(z_i,t_i,Y^{obs}_i)$:
\begin{eqnarray}
\mathbf{y}^{miss}_i|(z_i=k,t_i,\mathbf{y}^{obs}_i)&\sim& N_{J-q_i}(\boldsymbol\mu^{cond}_{ki},\boldsymbol\Sigma^{cond}_k),~\text{where}\nonumber\\
\boldsymbol\mu^{cond}_{ki}&=& \boldsymbol\mu^{miss} + \boldsymbol\Sigma_{12} \boldsymbol\Sigma_{22}^{-1}(\mathbf{y}^{obs}_i - \boldsymbol\mu^{obs})\\
\boldsymbol\Sigma^{cond}_k&=& \boldsymbol\Sigma_{11} - \boldsymbol\Sigma_{12}\boldsymbol\Sigma_{22}^{-1}\boldsymbol\Sigma_{21},~\text{where}\nonumber
\end{eqnarray}
$\boldsymbol\Sigma_k$ is partitioned into four sub-matrices $\boldsymbol\Sigma_{11},\boldsymbol\Sigma_{12},\boldsymbol\Sigma_{21}, \ \text{and} \ \boldsymbol\Sigma_{22}$ such that $\boldsymbol\Sigma_{11}$ is a $J-q_i \times J-q_i$ matrix containing the rows and columns of $\boldsymbol\Sigma_k$ corresponding to inicies of $\mathbf{y}_{i}$ where missingness occurs. Similarly, $\boldsymbol\Sigma_{12}$ is a $J-q_i \times q_i$ matrix containing the rows of $\boldsymbol\Sigma_k$ that correspond to missing indices of $\mathbf{y}_i$, but columns of $\boldsymbol\Sigma_k$ that correspond to observed indices of $\mathbf{y}_i$. The remaining partitions $\boldsymbol\Sigma_{21}, \ \text{and} \ \boldsymbol\Sigma_{22}$ are defined in the same manner. These results follow from conventional multivariate normal theory. An attractive feature of this imputation algorithm is that it avoids multiplicative run-time scaling in $m$, the number of imputations (Gelman \textit{et al.} 2013; Zhou and Reiter, 2010). Our approach also provides more opportunities to explore the missing data parameter space than does multiple imputation, since missing values are drawn at each MCMC iteration, and often in practice $n_{sim} >> m$, where $n_{sim}$ is the number of MCMC iterations. In Section 4, we conduct simulation studies to demonstrate that imputing the missing MSN responses improves inferences over complete case analysis.

\subsection{Bayesian Inference}

\subsubsection{Prior Specification}
We adopt a fully Bayesian inferential approach and assign prior distributions to all model parameters. Conveniently, all parameters admit conditionally conjugate priors, which greatly improves posterior computation via a data-augmented Gibbs sampler. For the MSN model component, we adopt a conditionally independent prior structure for $\boldsymbol\beta_k$ and $\boldsymbol\Sigma_k$, where $p(\boldsymbol\beta_k,\boldsymbol\Sigma_k) = p(\boldsymbol\Sigma_k)p(\boldsymbol\beta_k|\boldsymbol\Sigma_k)$. We choose the normal-inverse-Wishart distribution for $p(\boldsymbol\beta_k,\boldsymbol\Sigma_k)$ by specifying $\boldsymbol\Sigma_k \sim \text{IW}(\mathbf{V}_k,\nu_k)$ and $\boldsymbol\beta_k|\boldsymbol\Sigma_k \sim \text{N}_{Jp}(\mathbf{b}_k,\mathbf{I}_p \otimes \boldsymbol\Sigma_k)$. We assign the skewness parameters $\boldsymbol\psi_k$ a conjugate $\text{N}_J(\mathbf{m}_k,\mathbf{P}_k)$ prior. However, the updates of $\boldsymbol\beta_k$ and $\boldsymbol\psi_k$ can be combined into one step by defining the $(Jp + J) \times 1$ vector $\boldsymbol\beta^*_k = (\boldsymbol\beta_k,\boldsymbol\psi_k)^T$ for which we assume a $\text{N}_{Jp+J}(\mathbf{b}^*_k,\mathbf{I}_{(p+1)} \otimes \boldsymbol\Sigma_k)$ prior. For the multinomial logit model component, the regression parameters $\boldsymbol\delta_k$ are given a conjugate $\text{N}_r(\mathbf{d}_k, \mathbf{S}_k)$ prior for $k = 1,...,K-1$. 

We allow the normal-inverse-Wishart and multinomial hyperparameters to vary across clusters, though they may be shared across clusters in practice. An advantage of allowing for cluster-specific prior parameters is that \textit{a priori} knowledge of development trends can be incorporated into certain clusters while still allowing the parameters of other clusters to be almost entirely determined by the data. Additionally, prior information regarding the effect of certain covariates on development cluster membership can be incorporated in to the model by choosing informative values for $\mathbf{d}_k \ \text{and} \ \mathbf{S}_k$

\subsubsection{Posterior Inference} 
The above prior specification induces closed-form full conditionals that can be efficiently updated as part of a Gibbs sampler outlined below. Additional details, including derivations can be found in the Web Appendix. \textbf{[Think about the best way to organize this section. Maybe see my Bayesian Analysis paper for guidance? We can discuss next week.]}

\paragraph{P\'olya--Gamma Data Augmentation for $z_i$} The sampler begins by updating the latent cluster indicators $z_i$ $(i=1,\ldots,n)$ from its multinomial logit full conditional. To facilitate sampling, we adopt an efficient data-augmentation approach introduced by Polson \textit{et al.} (2013), which expresses the inverse-logit function as a mixture P\'olya--Gamma densities. A random variable $w$ is said to follow a P\'olya--Gamma distribution with parameters $b > 0$ and $c \in {\rm I\!R}$ if
$$w \sim \text{PG}(b,c) \stackrel{d}{=} \frac{1}{2\pi^2}\sum_{s=1}^{\infty}\frac{g_s}{(s-1/2)^2 + c^2/(4\pi^2)},$$
where $g_s \stackrel{iid}{\sim} \text{Ga}(b,1)$ for $s = 1,...,\infty$  
$$p(\boldsymbol\delta_k|\mathbf{z},\boldsymbol\delta_{h \ne k}) \propto p(\boldsymbol\delta_k) \prod_{i = 1}^{n} \pi_{ik}^{U_{ik}}(1-\pi_{ik})^{1-U_{ik}}$$
where $p(\boldsymbol\delta_k)$ denotes the prior distribution of $\boldsymbol\delta_k$, $U_{ik} = {1}_{z_i = k}$ is an indicator that subject $i$ belongs to cluster $k$, and $\pi_{ik}$ is defined as in Section 3.4. We can rewrite $\pi_{ik}$ as follows
$$\pi_{ik} = P(U_{ik} = 1) = \frac{e^{\mathbf{w}_i^T \boldsymbol\delta_k - {c}_{ik}}}{1 + e^{\mathbf{w}_i^T \boldsymbol\delta_k - {c}_{ik}}} = \frac{e^{\eta_{ik}}}{1 + e^{\eta_{ik}}}$$
where ${c}_{ik} = \log \sum_{h \ne k} e^{\mathbf{w}_i^T \boldsymbol\delta_{h}}$ and $\eta_{ik} = \mathbf{w}_i^T \boldsymbol\delta_k - {c}_{ik}$. We note that the sum $\sum_{h \ne k} e^{\mathbf{w}_i^T \boldsymbol\delta_{h}}$ includes the reference category, but since we fix $\boldsymbol\delta_K = \mathbf{0}$, we have $e^{\mathbf{w}_i^T \boldsymbol\delta_K} = 1$, and hence
$$c_{ik} = \log \sum_{h \ne k} e^{\mathbf{w}_i^T \boldsymbol\delta_{h}} = \log \left ( 1 + \sum_{h \notin \{k,K \}} e^{\mathbf{w}_i^T \boldsymbol\delta_{h}} \right )$$

We can use the quantities to re-express the full conditionals for $\boldsymbol\delta_k$ as
$$p(\boldsymbol\delta_k|\mathbf{Z},\boldsymbol\delta_{h \ne k}) \propto p(\boldsymbol\delta_k) \prod_{i = 1}^{n} \left (\frac{e^{\eta_{ik}}}{1 + e^{\eta_{ik}}} \right )^{U_{ik}} \left (\frac{1}{1 + e^{\eta_{ik}}} \right )^{1-U_{ik}} = p(\boldsymbol\delta_k) \prod_{i = 1}^n \frac{(e^{\eta_{ik}})^{U_{ik}}}{1 + e^{\eta_{ik}}}$$

which we note is essentially a logistic regression likelihood. We thus apply this P\'olya--Gamma data augmentation scheme to update each $\boldsymbol\delta_k$ ($k = 1,...,K-1$) one at a time based on the binary indicators $U_{ik}$.

\begin{itemize}

\item Emphasize that PG data augmentation for the multinomial model results in a PG mixture of experts model, which is a computationally efficient way to model edge weights.

\end{itemize}

\subsubsection{MCMC Algorithm}

\begin{algorithm}
\caption{Gibbs Sampler}
\begin{algorithmic}
\small
\linespread{0.5}
    \State $\mathbf{Define} \ n_{iter};\  n_{burn}; \ K; \ \boldsymbol\theta_{init}; \ \boldsymbol\theta_0$
    \State $n_{sim} := n_{iter} - n_{burn}$
    \State $\boldsymbol\theta := \boldsymbol\theta_{init}$
    \For {$\iota = 1,...,n_{sim}$}
        \State \textsc{I. Conditional Imputation}
          \For {$ i = 1,...,n$}
            \State $\mathbf{Draw} \ \mathbf{y}_i^{miss} \ \text{from} \ N_q(\boldsymbol\mu_i^{miss}, \boldsymbol\Sigma_i^{miss})$
            \State $\mathbf{y}_i := \mathbf{y}_i^{miss} \cup \mathbf{y}_i^{obs}$
         \EndFor
        \State \textsc{II. MSN Regression}
          \For {$ k = 1,...,K$}
            \State $n_k := \sum_{i = 1}^n {1}_{z_i = k}$
            \For {$i_k = 1,...,n_k$}
              \State $\mathbf{Draw}\  t_i \ \text{from} \ N_{[0,\infty)}(a_i,A)$
            \EndFor
            \State $\mathbf{X^*}_k := $ \texttt{cbind}$(\mathbf{X}_k,\mathbf{t}_k)$
            \State $\mathbf{Draw} \ \mathbf{B^*}_k \ \text{from} \ \text{MatNorm}(\mathbf{B}_k,\mathbf{L}_k^{-1},\boldsymbol\Sigma_k)$
            \State $\mathbf{Draw} \ \boldsymbol\Sigma_k \ \text{from} \ \text{InvWish}(\nu_k, \mathbf{V}_k)$
          \EndFor
        \State \textsc{III. Multinomial Logit}
          \For {$i = 1,...,n$}
            \For {$k = 1,...,K$}
              \State $\pi_{ik} := P(z_i = k|\mathbf{w}_i,\boldsymbol\delta_k)$
              \State $p_{ik} := P(\mathbf{y}_i|\boldsymbol\beta_k^{*T} \mathbf{x}^*_i,\boldsymbol\Sigma_k)$
            \EndFor
            \State $\mathbf{p}_{z_i} := \frac{\mathbf{p}_i \circ \boldsymbol\pi_i}{\mathbf{p}_i \cdot \boldsymbol\pi_i}$
            \State $\mathbf{Draw} \ z_i \ \text{from} \ \text{Categorical}(\mathbf{p}_{z_i})$
            \For {$k = 1,...,K-1$}
              \State $\mathbf{Draw} \ \boldsymbol\delta_k \ \text{from} \ N(\mathbf{M},\mathbf{S})$
            \EndFor
          \EndFor
        \State $\boldsymbol\theta := \{\mathbf{B^*}, \boldsymbol\Sigma, \mathbf{Z}, \boldsymbol\delta \}$
        \State $\mathbf{Store} \ \boldsymbol\theta$
	  \EndFor
\end{algorithmic}
\end{algorithm}


\subsubsection{Assessment of MCMC Convergence}

\subsubsection{Label Switching}

\newpage

\section{Simulation Studies}
\label{s:sim}

\subsection{Simulation to Compare to Multivariate Normal}

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-4}Model results for simulated data with n = 1500, k = 4, p = 1, h = 3, v = 1. 5000 iterations were run with a burn in of 1000. Missingness mechanism was MAR and P(miss) = 0}
\centering
\fontsize{8}{10}\selectfont
\begin{tabular}{llllllll}
\toprule
\multicolumn{2}{c}{ } & \multicolumn{2}{c}{Class 1} & \multicolumn{2}{c}{Class 2} & \multicolumn{2}{c}{Class 3} \\
\cmidrule(l{3pt}r{3pt}){3-4} \cmidrule(l{3pt}r{3pt}){5-6} \cmidrule(l{3pt}r{3pt}){7-8}
Model Component & Parameter & True & Est. (95\% CrI) & True & Est. (95\% CrI) & True & Est. (95\% CrI)\\
\midrule
\addlinespace[0.3em]
\multicolumn{8}{l}{\textbf{ }}\\
\hspace{1em}MVSN & $\beta_{0}$ & -3.21 & -3.34 (-3.8, -2.99) & -0.32 & -0.33 (-0.48, -0.14) & 3.35 & 3.33 (3.22, 3.44)\\
\hspace{1em}Regression & $\beta_{1}$ & -3.08 & -3.3 (-3.73, -2.87) & -0.75 & -0.72 (-0.87, -0.52) & 2.6 & 2.5 (2.39, 2.6)\\
\hspace{1em} & $\beta_{2}$ & -2.97 & -3.18 (-3.58, -2.76) & -0.45 & -0.44 (-0.58, -0.26) & 3.43 & 3.42 (3.31, 3.53)\\
\hspace{1em} & $\beta_{3}$ & -2.91 & -3.08 (-3.49, -2.68) & -0.66 & -0.68 (-0.83, -0.48) & 3.04 & 2.98 (2.87, 3.09)\\
\addlinespace[0.3em]
\multicolumn{8}{l}{\textbf{ }}\\
\hspace{1em} & $\sigma_{11}$ & 1 & 0.95 (0.84, 1.02) & 1 & 1 (0.89, 1.11) & 1 & 1.06 (0.97, 1.16)\\
\hspace{1em} & $\sigma_{12}$ & 0.74 & 0.7 (0.59, 0.76) & 0.68 & 0.68 (0.59, 0.78) & -0.45 & -0.41 (-0.47, -0.36)\\
\hspace{1em} & $\sigma_{13}$ & 0.74 & 0.69 (0.58, 0.75) & -0.16 & -0.13 (-0.2, -0.06) & 0.82 & 0.88 (0.79, 0.97)\\
\hspace{1em} & $\sigma_{14}$ & 0.98 & 0.93 (0.81, 0.99) & 0.64 & 0.65 (0.56, 0.75) & 0.7 & 0.75 (0.67, 0.83)\\
\hspace{1em} & $\sigma_{22}$ & 1 & 0.94 (0.82, 1.01) & 1 & 1.03 (0.93, 1.13) & 1 & 1.07 (0.99, 1.16)\\
\hspace{1em} & $\sigma_{23}$ & 0.83 & 0.79 (0.67, 0.85) & -0.43 & -0.4 (-0.46, -0.34) & -0.66 & -0.62 (-0.68, -0.57)\\
\hspace{1em} & $\sigma_{24}$ & 0.81 & 0.77 (0.66, 0.83) & 0.63 & 0.67 (0.58, 0.77) & 0.01 & 0.07 (0.01, 0.13)\\
\hspace{1em} & $\sigma_{33}$ & 1 & 0.96 (0.84, 1.03) & 1 & 1 (0.91, 1.09) & 1 & 1.05 (0.96, 1.15)\\
\hspace{1em} & $\sigma_{34}$ & 0.85 & 0.81 (0.69, 0.87) & 0.15 & 0.15 (0.08, 0.23) & 0.59 & 0.64 (0.56, 0.72)\\
\hspace{1em} & $\sigma_{44}$ & 1 & 0.95 (0.83, 1.01) & 1 & 1.02 (0.92, 1.13) & 1 & 1.06 (0.97, 1.15)\\
\addlinespace[0.3em]
\multicolumn{8}{l}{\textbf{ }}\\
\hspace{1em} & $\psi_{1}$ & -0.33 & -0.33 (-0.62, 0.69) & 0.67 & 0.7 (0.46, 0.89) & -1 & -1.01 (-1.13, -0.87)\\
\hspace{1em} & $\psi_{2}$ & -0.33 & -0.32 (-0.61, 0.64) & 0.67 & 0.63 (0.38, 0.82) & -1 & -0.88 (-1.01, -0.75)\\
\hspace{1em} & $\psi_{3}$ & -0.33 & -0.33 (-0.61, 0.69) & 0.67 & 0.64 (0.43, 0.82) & -1 & -1.01 (-1.14, -0.88)\\
\hspace{1em} & $\psi_{4}$ & -0.33 & -0.31 (-0.63, 0.67) & 0.67 & 0.7 (0.45, 0.89) & -1 & -0.94 (-1.07, -0.81)\\
\addlinespace[0.3em]
\multicolumn{8}{l}{\textbf{ }}\\
\hspace{1em}Multinom. & $\delta_{11}$ & 0.9 & 0.88 (0.81, 0.95) & 0.9 & 0.88 (0.81, 0.95) & 0.9 & 0.88 (0.81, 0.95)\\
\hspace{1em} & $\delta_{12}$ & 0.23 & 0.22 (0.14, 0.3) & 0.23 & 0.22 (0.14, 0.3) & 0.23 & 0.22 (0.14, 0.3)\\
\addlinespace[0.3em]
\multicolumn{8}{l}{\textbf{ }}\\
\hspace{1em}Clustering & $\pi_l$ & 0.28 & 0.28 (0.27, 0.28) & 0.42 & 0.43 (0.42, 0.43) & 0.3 & 0.3 (0.3, 0.3)\\
\bottomrule
\end{tabular}
\end{table}


\subsection{Simulation to Compare Imputation Methods}

\subsection{Simulation to Assess Sensitivity to Misspecified K}

\newpage

\section{Application}
\label{s:app}

\begin{itemize}

\item Include both time varying and non-time varying covariates for the within cluster covariate set. 

\end{itemize}

\newpage

\section{Discussion}
\label{s:discuss}

\textit{\textbf{Discuss non-ignorable missingness here}}


\begin{itemize}

\item Discuss how we handle non-ignorable missingness 

\end{itemize}


\section{Appendix}

Put your final comments here. 

%  The \backmatter command formats the subsequent headings so that they
%  are in the journal style.  Please keep this command in your document
%  in this position, right after the final section of the main part of 
%  the paper and right before the Acknowledgements, Supplementary Materials,
%  and References sections. 

\backmatter

%  This section is optional.  Here is where you will want to cite
%  grants, people who helped with the paper, etc.  But keep it short!

\section*{Acknowledgements}

%  If your paper refers to supplementary web material, then you MUST
%  include this section!!  See Instructions for Authors at the journal
%  website http://www.biometrics.tibs.org

\section*{Supplementary Materials}

\newpage

\subsection{Glossary of Notation}

\begin{itemize}

    \item $\mathbf{Y}$: A $n \times J$ matrix containing all multivariate skew-normal  outcomes such that $y_{ij}$ is the $j^{th}$ outcome observed for subject $i$, where $i = 1,...,n$ and $j = 1,...J$.
    
    \item $\mathbf{X}$: A $n \times P$ matrix containing all multivariate skew-normal regression covariates such that $x_{ip}$ is the $p^{th}$ covariate value for subject $i$, where $i = 1,...,n$ and $p = 1,...P$.
    
    \item $\mathbf{B}$: A $P \times J$ matrix containing all multivariate skew-normal regression coefficients such that $\mathbf{B} = \left [ \boldsymbol\beta_1,...,\boldsymbol\beta_J \right ]$, where $\beta_{pj}$ is interpreted as the effect of covariate $p$ on outcome $j$ for $p = 1,...,P$ and $j = 1,...,J$.
    
    \item $\mathbf{E}$: A $n \times J$ matrix of error terms in the multivariate skew-normal regression model component. $\mathbf{E}$ is made up of row vectors $\boldsymbol\epsilon_i = (\epsilon_{i1},...,\epsilon_{iJ})$, where $ \boldsymbol\epsilon_i \stackrel{iid}{\sim} N_J(0, \boldsymbol\Sigma)$ for $i = 1,...,n$.
    
    \item $\boldsymbol\Sigma$: A $J \times J$ covariance matrix that defines the correlation between the $p$ multivariate normal outcomes. 
    
    \item $\boldsymbol\Omega$: A $J \times J$ covariance scale matrix that defines the correlation between the $p$ multivariate skew-normal outcomes. 
    
    \item $\boldsymbol\psi$: A $J \times 1$ vector containing the skewness parameter for each outcome.
    
    \item $\boldsymbol\alpha$: A $J \times 1$ vector containing the skewness parameter for each outcome.
    
    \item $\mathbf{t}$: An $n \times 1$ vector of truncated normal random effects used in the stochastic representation of the multivariate skew-normal distribution. For $i = 1,...,n$, $t_i \stackrel{iid}{\sim}T_{[0,\infty)}(0,1)$
    
    \item $\mathbf{X}^*$: A $n \times (P + 1)$ matrix constructed by column binding $\mathbf{t}$ to $\mathbf{X}$
    
    \item $\mathbf{B}^*$: A $(P+1) \times J$ matrix constructed by row binding $\boldsymbol\psi^T$ to $\mathbf{B}$.

\end{itemize}

\newpage
\subsection{Derivation of Full Conditional Distributions}

\subsubsection{Multivariate Skew-Normal Regression}

Without loss of generality, we derive the full conditional distributions for the multivariate skew-normal regression model component under the assumption that all observations belong to a single cluster. To make the extension to the case where more than one cluster is specified, simply apply these distributional forms to cluster specific parameters and data. Finally, we assume for the moment that we have complete data for all outcomes for each subject. We extend consider the case of missing data in section (INSERT SECTION).

The multivariate skew-normal regression model can be written as follows in matrix form. 
$$\mathbf{Y} = \mathbf{X}\mathbf{B} + \mathbf{t} \boldsymbol\psi^T + \mathbf{E} = \mathbf{X}^* \mathbf{B}^* + \mathbf{E}$$
The matrix $\mathbf{Y}$ is of dimension $n \times J$. For convenience, we define $\mathbf{X}^*$ as a $n \times (P + 1)$ matrix constructed by column binding $\mathbf{t}$ to $\mathbf{X}$, and $\mathbf{B}^*$ as a $(P+1) \times J$ matrix constructed by row binding $\boldsymbol\psi^T$ to $\mathbf{B}$. We assume that $t_i \stackrel{iid}{\sim}T_{[0,\infty})(0,1)$ and that $\mathbf{E}$ is made of row vectors $\boldsymbol\epsilon_i = (\epsilon_{i1},...,\epsilon_{iJ})$ for $i = 1,...,n$, where $ \boldsymbol\epsilon_i \stackrel{iid}{\sim} N_J(0, \boldsymbol\Sigma)$.

The conditional likelihood for this model is given below. 
$$p(\mathbf{Y}|\mathbf{X}^*,\mathbf{B}^*,\boldsymbol\Sigma) \propto |\boldsymbol\Sigma|^{-n/2}\exp \left \{ -\frac{1}{2} \tr(\mathbf{Y} - \mathbf{X}^* \mathbf{B}^*)^T(\mathbf{Y} - \mathbf{X}^* \mathbf{B}^*)\boldsymbol\Sigma^{-1} \right \}$$

We choose conjugate priors for $\mathbf{B}^*$ and $\boldsymbol\Sigma$ as follows. 
$$\boldsymbol\Sigma \sim \text{inverse-Wishart}(\mathbf{V}_0,\nu_0)$$
$$\mathbf{B}^*|\boldsymbol\Sigma \sim MatNorm_{(m+1), p}(\mathbf{B}_0^*,\mathbf{L}_0^{-1},\boldsymbol\Sigma)$$

We now derive the joint posterior distribution of the parameters $\mathbf{B}^*$ and $\boldsymbol\Sigma$.
\begin{align*} 
p(\mathbf{B}^*,\boldsymbol\Sigma|\mathbf{X}^*,\mathbf{Y}) & \propto  p(\mathbf{Y}|\mathbf{X}^*,\mathbf{B}^*,\boldsymbol\Sigma)p(\mathbf{B}^*|\boldsymbol\Sigma)p(\boldsymbol\Sigma)\\
&  \propto |\boldsymbol\Sigma|^{-n/2}\exp \left \{ -\frac{1}{2} \tr \left [(\mathbf{Y} - \mathbf{X^*}\mathbf{B}^*)^T(\mathbf{Y} - \mathbf{X^*}\mathbf{B}^*)\boldsymbol\Sigma^{-1} \right ]\right \} \\
& \times |\boldsymbol\Sigma|^{-(P+1)/2} \exp \left \{ -\frac{1}{2} \tr \left [(\mathbf{B}^* - \mathbf{B}^*_0)^T \mathbf{L}_0(\mathbf{B}^* - \mathbf{B}^*_0)\boldsymbol\Sigma^{-1} \right ]\right \} \\
& \times |\boldsymbol\Sigma|^{(\nu_0 + J + 1)/2} \exp \left \{ -\frac{1}{2} \tr (\mathbf{V}_0 \boldsymbol\Sigma^{-1}) \right \}
\end{align*}



\subsubsection{Multinomial Logit Regression}

\subsubsection{Multivariate Normal Conditional Imputation}

The multivariate normal conditional imputation derivations are given for a single cluster without loss of generality. In practice, the data and parameters in this section would be replaced by cluster specific estimates in the case of clustering. 

For a given observation vector $\mathbf{y} \sim N_J(\boldsymbol\mu, \boldsymbol\Sigma)$, we allow for missingness in at most $J - 1$ of the multivariate outcomes through the use of a conditional imputation step embedded within our Gibbs sampler. Suppose $\mathbf{y}$ contains $q$ missing observations and can be partitioned into two vectors $\mathbf{y_{1}}$ and $\mathbf{y_{2}}$ such that $\mathbf{y_{1}}$ is a $q \times 1$ vector of missing observations and $\mathbf{y_{2}}$ is a $(J-q) \times 1$ vector of complete observations. Similarly, partition $\boldsymbol\mu$ and $\boldsymbol\Sigma$ as follows.
$$\boldsymbol\mu = \begin{bmatrix} \boldsymbol\mu_1 \\ \boldsymbol\mu_2 \end{bmatrix} \ \ \ \ \ \boldsymbol\Sigma = \begin{bmatrix} \boldsymbol\Sigma_{11} & \boldsymbol\Sigma_{12} \\ \boldsymbol\Sigma_{21} & \boldsymbol\Sigma_{22} \end{bmatrix}$$

We will use these quantities to derive the conditional distribution $f(\mathbf{y_1}|\mathbf{y_2},\boldsymbol\mu,\boldsymbol\Sigma)$. 
\begin{align*} 
f(\mathbf{y_1}|\mathbf{y_2},\boldsymbol\mu,\boldsymbol\Sigma) & \propto  f(\mathbf{y_1},\mathbf{y_2}|\boldsymbol\mu,\boldsymbol\Sigma) \\
& \propto \exp \left \{ -\frac{1}{2}(\mathbf{y} - \boldsymbol\mu)^T \boldsymbol\Sigma^{-1} (\mathbf{y} - \boldsymbol\mu) \right \} \\
& = \exp \left \{ -\frac{1}{2}\begin{bmatrix} \mathbf{y}_1 - \boldsymbol\mu_1 \\  \mathbf{y}_2 - \boldsymbol\mu_2 \end{bmatrix}^T \boldsymbol\Sigma^{-1} \begin{bmatrix} \mathbf{y}_1 - \boldsymbol\mu_1 \\  \mathbf{y}_2 - \boldsymbol\mu_2 \end{bmatrix} \right \} \\
& =  \exp \left \{ -\frac{1}{2}\begin{bmatrix} \mathbf{y}_1 - \boldsymbol\mu_1 \\  \mathbf{y}_2 - \boldsymbol\mu_2 \end{bmatrix}^T \begin{bmatrix} \boldsymbol\Sigma_{11} & \boldsymbol\Sigma_{12} \\ \boldsymbol\Sigma_{21} & \boldsymbol\Sigma_{22} \end{bmatrix}^{-1} \begin{bmatrix} \mathbf{y}_1 - \boldsymbol\mu_1 \\  \mathbf{y}_2 - \boldsymbol\mu_2 \end{bmatrix} \right \} \\
& = \exp \left \{ -\frac{1}{2}\begin{bmatrix} \mathbf{y}_1 - \boldsymbol\mu_1 \\  \mathbf{y}_2 - \boldsymbol\mu_2 \end{bmatrix}^T \begin{bmatrix} \boldsymbol\Sigma_{11}^* & \boldsymbol\Sigma_{12}^* \\ \boldsymbol\Sigma_{21}^* & \boldsymbol\Sigma_{22}^* \end{bmatrix} \begin{bmatrix} \mathbf{y}_1 - \boldsymbol\mu_1 \\  \mathbf{y}_2 - \boldsymbol\mu_2 \end{bmatrix} \right \} \\
& = \exp \left \{ -\frac{1}{2} \left[ (\mathbf{y}_1 - \boldsymbol\mu_{cond})^T \boldsymbol\Sigma_{cond}^{-1}(\mathbf{y}_1 - \boldsymbol\mu_{cond})\right]\right \} 
\end{align*}
$$\Rightarrow \mathbf{y_1}|\mathbf{y_2},\boldsymbol\mu,\boldsymbol\Sigma \sim N_q(\boldsymbol\mu_{cond},\boldsymbol\Sigma_{cond})$$
$$\boldsymbol\mu_{cond} = \boldsymbol\mu_1 + \boldsymbol\Sigma_{12}\boldsymbol\Sigma_{22}^{-1}(\mathbf{y}_2 - \boldsymbol\mu_2), \ \ \ \ \ \ \boldsymbol\Sigma_{cond} = \boldsymbol\Sigma_{11} - \boldsymbol\Sigma_{12}\boldsymbol\Sigma_{22}^{-1}\boldsymbol\Sigma_{21}$$

The block-wise inversion formula was used to invert $\boldsymbol\Sigma$ according to the following reparameterizations.
\begin{align*}
\boldsymbol\Sigma_{11}^* & = \boldsymbol\Sigma_{11}^{-1} + \boldsymbol\Sigma_{11}^{-1}\boldsymbol\Sigma_{12}(\boldsymbol\Sigma_{22} - \boldsymbol\Sigma_{21}\boldsymbol\Sigma_{11}^{-1}\boldsymbol\Sigma_{12})^{-1}\boldsymbol\Sigma_{21}\boldsymbol\Sigma_{11}^{-1}\\
\boldsymbol\Sigma_{12}^* & = -\boldsymbol\Sigma_{11} \boldsymbol\Sigma_{12}(\boldsymbol\Sigma_{22}-\boldsymbol\Sigma_{21}\boldsymbol\Sigma_{11}^{-1}\boldsymbol\Sigma_{12})^{-1}\\
\boldsymbol\Sigma_{21}^* & = -(\boldsymbol\Sigma_{22} - \boldsymbol\Sigma_{21}\boldsymbol\Sigma_{11}^{-1}\boldsymbol\Sigma_{12})^{-1}\boldsymbol\Sigma_{21}\boldsymbol\Sigma_{11}^{-1}\\
\boldsymbol\Sigma_{22}^* & = (\boldsymbol\Sigma_{22} - \boldsymbol\Sigma_{21}\boldsymbol\Sigma_{11}^{-1}\boldsymbol\Sigma_{12})^{-1}     
\end{align*}


%  Here, we create the bibliographic entries manually, following the
%  journal style.  If you use this method or use natbib, PLEASE PAY
%  CAREFUL ATTENTION TO THE BIBLIOGRAPHIC STYLE IN A RECENT ISSUE OF
%  THE JOURNAL AND FOLLOW IT!  Failure to follow stylistic conventions
%  just lengthens the time spend copyediting your paper and hence its
%  position in the publication queue should it be accepted.

%  We greatly prefer that you incorporate the references for your
%  article into the body of the article as we have done here 
%  (you can use natbib or not as you choose) than use BiBTeX,
%  so that your article is self-contained in one file.
%  If you do use BiBTeX, please use the .bst file that comes with 
%  the distribution.  In this case, replace the thebibliography
%  environment below by 
%
%  \bibliographystyle{biom} 
% \bibliography{mybibilo.bib}

\newpage

\begin{thebibliography}{}

\bibitem{ } Arellano‐-Valle RB, Azzalini A. On the unification of families of skew‐normal distributions. \textit{Scandinavian Journal of Statistics}. 2006 Sep;33(3):561-74.

\bibitem{ } Azzalini A. A class of distributions which includes the normal ones. \textit{Scandinavian journal of statistics}. 1985 Jan 1:171-8.

\bibitem{ } Azzalini, A. and Dalla Valle, A. (1996). The multivariate skew normal distribution. Biometrika 83, 715–726.

\bibitem{ } Chen JT, Gupta AK. Matrix variate skew normal distributions. \textit{Statistics}. 2005 Jun 1;39(3):247-53.

\bibitem{ } Neelon SE, \O stbye T, Bennett GG, Kravitz RM, Clancy SM, Stroo M, Iversen E, Hoyo C. Cohort profile for the Nurture Observational Study examining associations of multiple caregivers on infant growth in the Southeastern USA. \textit{BMJ Open}. 2017 Feb 1;7(2):e013939.

\bibitem{ } Franczak BC, Tortora C, Browne RP, McNicholas PD. Unsupervised learning via mixtures of skewed distributions with hypercube contours. \textit{Pattern Recognition Letters}. 2015 Jun 1;58:69-76.

\bibitem{ } Fr\"{u}hwirth-Schnatter S, Pyne S. Bayesian inference for finite mixtures of univariate and multivariate skew-normal and skew-t distributions. \textit{Biostatistics}. 2010 Jan 27;11(2):317-36.

\bibitem{ } Ganjali M, Baghfalaki T. A Bayesian shared parameter model for analysing longitudinal skewed responses with nonignorable dropout. \textit{International Journal of Statistics in Medical Research}. 2014 Apr 1;3(2):103.

\bibitem{ } Gelman A, Stern HS, Carlin JB, Dunson DB, Vehtari A, Rubin DB. Bayesian data analysis. \textit{Chapman and Hall/CRC}; 2013 Nov 27.

\bibitem{ } Gelman A, Hwang J, Vehtari A. Understanding predictive information criteria for Bayesian models. \textit{Statistics and Computing}. 2014 Nov 1;24(6):997-1016.

\bibitem{ } Holmes CC, Held L. Bayesian auxiliary variable models for binary and multinomial regression. \textit{Bayesian Analysis}. 2006;1(1):145-68.

\bibitem{ } Lagona F, Picone M. Model-based clustering of multivariate skew data with circular components and missing values. \textit{Journal of Applied Statistics}. 2012 May 1;39(5):927-45.

\bibitem{ } Lanza ST, Rhoades BL. Latent class analysis: an alternative perspective on subgroup analysis in prevention and treatment. \textit{Prevention Science}. 2013 Apr 1;14(2):157-68.

\bibitem{ } Lee SX, McLachlan GJ. Model-based clustering and classification with non-normal mixture distributions. Statistical Methods \& Applications. 2013 Nov 1;22(4):427-54.

\bibitem{ } Lee SX, Mclachlan GJ. On mixtures of skew normal and skew $t$-distributions. \textit{Advances in Data Analysis and Classification}. 2013 Sep 1;7(3):241-66.

\bibitem{ } Lin TI, Wang WL, McLachlan GJ, Lee SX. Robust mixtures of factor analysis models using the restricted multivariate skew-t distribution. \textit{Statistical Modelling}. 2018 Feb;18(1):50-72.

\bibitem{ } Luo S, Lawson AB, He B, Elm JJ, Tilley BC. Bayesian multiple imputation for missing multivariate longitudinal data from a Parkinson's disease clinical trial. \textit{Statistical Methods in Medical Research}. 2016 Apr;25(2):821-37.

\bibitem{ } Melnykov V, Maitra R. Finite mixture models and model-based clustering. \textit{Statistics Surveys}. 2010;4:80-116.

\bibitem{ } Polson NG, Scott JG, Windle J. Bayesian inference for logistic models using P\'olya –- Gamma latent variables. Journal of the American statistical Association. 2013 Dec 1;108(504):1339-49.

\bibitem{ } Tiao GC, Zellner A. On the Bayesian estimation of multivariate regression. \textit{Journal of the Royal Statistical Society}: Series B (Methodological). 1964 Jul;26(2):277-85.

\bibitem{ } Viroli C. Finite mixtures of matrix normal distributions for classifying three-way data. \textit{Statistics and Computing}. 2011 Oct 1;21(4):511-22.

\bibitem{ } Vrbik I, Mcnicholas PD. Parsimonious skew mixture models for model-based clustering and classification. \textit{Computational Statistics \& Data Analysis}. 2014 Mar 1;71:196-210.

\bibitem{ } Zeller CB, Cabral CR, Lachos VH. Robust mixture regression modeling based on scale mixtures of skew-normal distributions. \textit{Test}. 2016 Jun 1;25(2):375-96.

\bibitem{ } Zhou X, Reiter JP. A note on Bayesian inference after multiple imputation. \textit{The American Statistician}. 2010 May 1;64(2):159-63.

\end{thebibliography}

% \appendix

%  To get the journal style of heading for an appendix, mimic the following.


\label{lastpage}

\end{document}
